{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210417_선형회귀_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUuiOoqpZB5JeMNs7Epil3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hodoosol/Start_DL_With_Pytorch/blob/main/Chap3_1_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CC9WulEaHWC"
      },
      "source": [
        "# 1. 선형 회귀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf9pCMdYTPM4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hURATlAYTBB4",
        "outputId": "f454e532-6407-455b-8d57-50c8af2b9a93"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4f868cb910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTx7xgzJS7QE"
      },
      "source": [
        "# 변수 선언\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMaBoXrkTTiw",
        "outputId": "d084342c-ad8d-4813-8311-b9f3c2e8154b"
      },
      "source": [
        "# 크기 출력\n",
        "print(x_train)\n",
        "print(x_train.shape)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF3T7s14TeM_",
        "outputId": "63bdd938-4f2a-457c-b47f-cbf5fef5b806"
      },
      "source": [
        "print(y_train)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.],\n",
            "        [4.],\n",
            "        [6.]])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JICe5cRbTgz2",
        "outputId": "37b33a49-71fd-4937-a578-fd0467a8e433"
      },
      "source": [
        "# 가중치와 편향을 초기화해보자.\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "print(W)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtasfVMdTpg4",
        "outputId": "2d98701a-2f43-4ec9-a645-1b855bde4ba3"
      },
      "source": [
        "b = torch.zeros(1, requires_grad=True)\n",
        "print(b)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79p5A7zVTuj6"
      },
      "source": [
        "zeros를 사용해 가중치와 편향을 0으로 초기화 하고, \n",
        "\n",
        "requires_grad=True 를 통해 w와 b가 학습을 통해 값이 변경되는 변수임을 명시한다.\n",
        "\n",
        "현재 가중치 W와 b는 둘 다 0이므로 직선의 방정식은\n",
        "\n",
        "y = 0 * x + 0 이 된다. 적절하지 않다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI4xpldZTtkF",
        "outputId": "662e5290-0cf0-4f48-c29f-5fcf3df62450"
      },
      "source": [
        "# 가설 세우기\n",
        "hypothesis = x_train * W + b\n",
        "print(hypothesis)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3IoQxJUUcPM"
      },
      "source": [
        "수식적으로 단순히 '오차 = 실제값 - 예측값'으로 정의하면 오차값이 음수가 되는 경우가 생기므로 정확한 오차의 크기를 측정할 수 없다. \n",
        "\n",
        "따라서 오차를 그냥 전부 더하는 것이 아니라, \n",
        "\n",
        "각 오차들을 제곱해준 뒤에 전부 더하는 평균 제곱 오차(Mean Squared Error, MSE)로 비용함수를 만들어 보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1S4JUd4UxKz"
      },
      "source": [
        "기울기 w가 무한대로 커지면 커질 수록 cost의 값 또한 무한대로 커지고, 반대로 w가 무한대로 작아져도 cost의 값은 무한대로 커진다. \n",
        "\n",
        "따라서 경사하강법을 사용하여 비용 함수를 미분했을 때의 접선의 기울기가 0이 되는 지점을 찾아야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sJd1x94T7_W",
        "outputId": "a2420b88-5d8a-45e9-e4f5-2f376086e38c"
      },
      "source": [
        "# 비용 함수 선언하기\n",
        "cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "print(cost)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8OCl91QVLXe"
      },
      "source": [
        "# 경사 하강법 구현하기\n",
        "## SGD는 경사 하강법의 일종이며 lr은 learning rate를 의미한다.\n",
        "## 학습 대상인 W와 b가 SGD의 입력이 된다.\n",
        "\n",
        "optimizer = optim.SGD([W, b], lr = 0.01)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvSK9hulVotX"
      },
      "source": [
        "# optimizer.zero_grad()를 실행함으로써 미분을 통해 얻은 기울기를 0응로 초기화 한다.\n",
        "# 기울기를 초기화해야지만 새로운 가중치 편향에 대한 새로운 기울기를 구할 수 있다.\n",
        "optimizer.zero_grad()"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VyIyWSiVnsc"
      },
      "source": [
        "# 그 다음 cost.backward() 함수를 호출하면 W와 b에 대한 기울기가 계산된다.\n",
        "# 비용 함수를 미분하여 gradient 계산\n",
        "cost.backward()"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUjlyti9V_q7"
      },
      "source": [
        "# optimizer.step()을 호출하여 인수로 들어갔던 W와 b에서\n",
        "# 리턴되는 변수들의 기울기에 학습률을 곱하여 빼줌으로써 업데이트한다.\n",
        "\n",
        "optimizer.step()"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9Q-l3hWYes"
      },
      "source": [
        "전체 코드 정리\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wcM_i_VWR_3",
        "outputId": "b9c2ca8d-1287-4f53-a714-5b8b237740d5"
      },
      "source": [
        "# 데이터\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr = 0.01)\n",
        "\n",
        "# 원하는 만큼 경사 하강법 반복\n",
        "nb_epochs = 2000\n",
        "\n",
        "for epoch in range(nb_epochs + 1) :\n",
        "  # H(x) 계산\n",
        "  hypothesis = x_train * W + b\n",
        "\n",
        "  # cost 계산\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # 100번마다 로그 출력\n",
        "  if epoch % 100 == 0 :\n",
        "    print('Epoch {:4d} / {} W : {:.3f}, b{:.3f} Cost : {:.6f}'\n",
        "    .format(epoch, nb_epochs, W.item(), b.item(), cost.item()))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0 / 2000 W : 0.187, b0.080 Cost : 18.666666\n",
            "Epoch  100 / 2000 W : 1.746, b0.578 Cost : 0.048171\n",
            "Epoch  200 / 2000 W : 1.800, b0.454 Cost : 0.029767\n",
            "Epoch  300 / 2000 W : 1.843, b0.357 Cost : 0.018394\n",
            "Epoch  400 / 2000 W : 1.876, b0.281 Cost : 0.011366\n",
            "Epoch  500 / 2000 W : 1.903, b0.221 Cost : 0.007024\n",
            "Epoch  600 / 2000 W : 1.924, b0.174 Cost : 0.004340\n",
            "Epoch  700 / 2000 W : 1.940, b0.136 Cost : 0.002682\n",
            "Epoch  800 / 2000 W : 1.953, b0.107 Cost : 0.001657\n",
            "Epoch  900 / 2000 W : 1.963, b0.084 Cost : 0.001024\n",
            "Epoch 1000 / 2000 W : 1.971, b0.066 Cost : 0.000633\n",
            "Epoch 1100 / 2000 W : 1.977, b0.052 Cost : 0.000391\n",
            "Epoch 1200 / 2000 W : 1.982, b0.041 Cost : 0.000242\n",
            "Epoch 1300 / 2000 W : 1.986, b0.032 Cost : 0.000149\n",
            "Epoch 1400 / 2000 W : 1.989, b0.025 Cost : 0.000092\n",
            "Epoch 1500 / 2000 W : 1.991, b0.020 Cost : 0.000057\n",
            "Epoch 1600 / 2000 W : 1.993, b0.016 Cost : 0.000035\n",
            "Epoch 1700 / 2000 W : 1.995, b0.012 Cost : 0.000022\n",
            "Epoch 1800 / 2000 W : 1.996, b0.010 Cost : 0.000013\n",
            "Epoch 1900 / 2000 W : 1.997, b0.008 Cost : 0.000008\n",
            "Epoch 2000 / 2000 W : 1.997, b0.006 Cost : 0.000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W39f9EG5Y94h"
      },
      "source": [
        "optimizer.zero_grad() 가 필요한 이유 ?\n",
        "\n",
        "파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX1oEltjXu4o",
        "outputId": "58d8e50b-95c6-4939-86dd-e4ab9d27eeb6"
      },
      "source": [
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "nb_epochs = 20\n",
        "\n",
        "for epoch in range(nb_epochs + 1) :\n",
        "  z = 2 * w\n",
        "\n",
        "  z.backward()\n",
        "  print('수식을 w로 미분한 값 : {}'.format(w.grad))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 2.0\n",
            "수식을 w로 미분한 값 : 4.0\n",
            "수식을 w로 미분한 값 : 6.0\n",
            "수식을 w로 미분한 값 : 8.0\n",
            "수식을 w로 미분한 값 : 10.0\n",
            "수식을 w로 미분한 값 : 12.0\n",
            "수식을 w로 미분한 값 : 14.0\n",
            "수식을 w로 미분한 값 : 16.0\n",
            "수식을 w로 미분한 값 : 18.0\n",
            "수식을 w로 미분한 값 : 20.0\n",
            "수식을 w로 미분한 값 : 22.0\n",
            "수식을 w로 미분한 값 : 24.0\n",
            "수식을 w로 미분한 값 : 26.0\n",
            "수식을 w로 미분한 값 : 28.0\n",
            "수식을 w로 미분한 값 : 30.0\n",
            "수식을 w로 미분한 값 : 32.0\n",
            "수식을 w로 미분한 값 : 34.0\n",
            "수식을 w로 미분한 값 : 36.0\n",
            "수식을 w로 미분한 값 : 38.0\n",
            "수식을 w로 미분한 값 : 40.0\n",
            "수식을 w로 미분한 값 : 42.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_EQGPK-aLd2"
      },
      "source": [
        "# 2. 자동 미분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdQS0C-DaOSE"
      },
      "source": [
        "임의로 2w^2 + 5라는 식을 세워보고, w에 대해서 미분해보자.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LsJ970CZaGc"
      },
      "source": [
        "# 값이 2인 임의의 스칼라 텐서 w를 선언, 이 텐서에 대한 기울기를 저장\n",
        "w = torch.tensor(2.0, requires_grad=True)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3tmY-Djaf_Z"
      },
      "source": [
        "# 수식 정의하기\n",
        "y = w ** 2\n",
        "z = 2 * y + 5"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHcehovidVRl"
      },
      "source": [
        "# 수식 z를 w에 대해서 미분하기. backward()는 해당 수식의 w에 대한 기울기를 계산.\n",
        "z.backward()"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXRDscl4dfb2",
        "outputId": "405c24c0-f348-4def-e0a1-bab725686e47"
      },
      "source": [
        "# w.grad를 출력하면 w가 속한 수식을 w로 미분한 값을 확인할 수 있다.\n",
        "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNr691mHdoXN",
        "outputId": "b0c0f4ac-9b41-4548-b901-1deaa6f138bb"
      },
      "source": [
        "a = 2 * (w ** 2) + 5\n",
        "a.backward()\n",
        "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 16.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxzlxRFLeAE5"
      },
      "source": [
        "y = w ** 4\n",
        "d = 2 * y + 5\n",
        "d.backward()"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXxa1ij1gdWZ",
        "outputId": "32a186d9-6036-4af3-dac1-3ee672f5aa39"
      },
      "source": [
        "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 80.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8akx0Hi06WTq",
        "outputId": "121110af-37f3-4651-b589-8f887acbd567"
      },
      "source": [
        "w79 = torch.tensor(2.0, requires_grad=True)\n",
        "y79 = w79 ** 2\n",
        "z79 = 2 * y79 + 5\n",
        "\n",
        "z79.backward()\n",
        "print('수식을 w로 미분한 값 : {}'.format(w79.grad))"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw3kAgfb6kZD",
        "outputId": "8dc9f763-47bb-4e14-8166-fd98a4446631"
      },
      "source": [
        "w80 = torch.tensor(2.0, requires_grad=True)\n",
        "y80 = w80 ** 2\n",
        "z80 = 2 * (w80 ** 2) + 5\n",
        "\n",
        "z80.backward()\n",
        "print('수식을 w로 미분한 값 : {}'.format(w80.grad))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJYkk-9hhhfv"
      },
      "source": [
        "# 3. 다중 선형 회귀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPsGU7dTh8iQ"
      },
      "source": [
        "다수의 x로부터 y를 예측하는 다중 선형 회귀를 공부해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abKLopOXiSC5"
      },
      "source": [
        "3개의 퀴즈 점수로부터 최종 점수를 예측하는 모델 만들기.\n",
        "\n",
        "독립 변수 x의 개수가 3개 이므로\n",
        "\n",
        "H(x) = w1x1 + w2x2 + w3x3 + b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edQBRffDgzBq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmgWZPFzibKK",
        "outputId": "67330cd8-c8cd-4677-9183-86282e048bea"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4f868cb910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR0dkGrKid0X"
      },
      "source": [
        "# 훈련 데이터\n",
        "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
        "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
        "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ_66dX9ijdk"
      },
      "source": [
        "# 가중치 w와 b를 선언. w도 3개 선언해야 한다.\n",
        "w1 = torch.zeros(1, requires_grad=True)\n",
        "w2 = torch.zeros(1, requires_grad=True)\n",
        "w3 = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZECEiBzKiwfo",
        "outputId": "695b83d2-a1aa-4281-e366-167725fe2396"
      },
      "source": [
        "# 가설, 비용 함수, 옵티마이저를 선언한 후에 경사 하강법을 1천회 반복한다.\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
        "        ))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
            "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n",
            "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n",
            "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n",
            "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n",
            "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n",
            "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
            "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n",
            "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n",
            "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
            "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpEhgfnSlH-y"
      },
      "source": [
        "그러나 x의 개수가 3개가 아닌 1000개라면 ??\n",
        "\n",
        "x_train1 ... x_train1000 을 전부 선언하고\n",
        "\n",
        "w1 ... w1000 을 전부 선언해야 한다.\n",
        "\n",
        "이를 해결하기 위해 행렬 곱셈 연산(벡터의 내적)을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipWBx-g5jp04"
      },
      "source": [
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  80], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6s8ZOFhmBCi",
        "outputId": "b1fcba49-64d6-4ef5-db30-ed4057f8bc17"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3])\n",
            "torch.Size([5, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdM-VRT_mDsv"
      },
      "source": [
        "# 가중치와 편향 선언\n",
        "W = torch.zeros((3, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXco4AiRmOcA"
      },
      "source": [
        "행렬의 곱셈이 성립되려면\n",
        "\n",
        "좌측에 있는 행렬의 열 크기 == 우측에 있는 행렬의 행 크기\n",
        "\n",
        "x_train은 (5, 3)dlrh w는 (3, 1)이므로 가능해진다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL7L5ZZpmKNt"
      },
      "source": [
        "# 가설 선언하기 _ 행렬곱 matmul 사용\n",
        "hypothesis = x_train.matmul(W) + b"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpewyHFkm28U"
      },
      "source": [
        "전체 코드 정리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxIN2KMimqPn",
        "outputId": "ea4436cc-99f5-4c64-a46b-a0ce90bc2c78"
      },
      "source": [
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  80], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros((3, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.SGD([W, b], lr = 1e-5)\n",
        "\n",
        "nb_epochs = 20\n",
        "\n",
        "for epoch in range(nb_epochs + 1) :\n",
        "  # H(x) 계산\n",
        "  # 편향 b는 브로드 캐스팅되어 각 샘플에 더해진다.\n",
        "  hypothesis = x_train.matmul(W) + b\n",
        "  \n",
        "  # cost 계산\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
        "    ))"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
            "Epoch    1/20 hypothesis: tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]) Cost: 9537.694336\n",
            "Epoch    2/20 hypothesis: tensor([104.5421, 125.6208, 119.2478, 134.7861,  95.8280]) Cost: 3069.590820\n",
            "Epoch    3/20 hypothesis: tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]) Cost: 990.670288\n",
            "Epoch    4/20 hypothesis: tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]) Cost: 322.481873\n",
            "Epoch    5/20 hypothesis: tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]) Cost: 107.717064\n",
            "Epoch    6/20 hypothesis: tensor([148.9423, 178.9730, 169.8976, 192.0301, 136.5279]) Cost: 38.687496\n",
            "Epoch    7/20 hypothesis: tensor([151.1574, 181.6346, 172.4254, 194.8856, 138.5585]) Cost: 16.499043\n",
            "Epoch    8/20 hypothesis: tensor([152.4131, 183.1435, 173.8590, 196.5043, 139.7097]) Cost: 9.365656\n",
            "Epoch    9/20 hypothesis: tensor([153.1250, 183.9988, 174.6723, 197.4217, 140.3625]) Cost: 7.071114\n",
            "Epoch   10/20 hypothesis: tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]) Cost: 6.331847\n",
            "Epoch   11/20 hypothesis: tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]) Cost: 6.092532\n",
            "Epoch   12/20 hypothesis: tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0613]) Cost: 6.013817\n",
            "Epoch   13/20 hypothesis: tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]) Cost: 5.986785\n",
            "Epoch   14/20 hypothesis: tensor([154.0017, 185.0517, 175.6785, 198.5500, 141.1671]) Cost: 5.976325\n",
            "Epoch   15/20 hypothesis: tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]) Cost: 5.971208\n",
            "Epoch   16/20 hypothesis: tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]) Cost: 5.967835\n",
            "Epoch   17/20 hypothesis: tensor([154.0459, 185.1045, 175.7326, 198.6059, 141.2082]) Cost: 5.964969\n",
            "Epoch   18/20 hypothesis: tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]) Cost: 5.962291\n",
            "Epoch   19/20 hypothesis: tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]) Cost: 5.959664\n",
            "Epoch   20/20 hypothesis: tensor([154.0536, 185.1134, 175.7451, 198.6145, 141.2158]) Cost: 5.957089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us6aHUW9nke-"
      },
      "source": [
        "# 4. nn.Module로 구현하는 선형 회귀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3xLob2CncyM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puu2Gn_qnxeW",
        "outputId": "e3301ec6-2b49-4308-980f-cff20fc051c1"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4f868cb910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhUySIvdn2lH"
      },
      "source": [
        "# y = 2x를 가정한 상태에서 만들어진 데이터 (정답 W = 2, b = 0)\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RUnHAIsn-eV"
      },
      "source": [
        "# 선형 회귀 모델 구현 _ 모델 선언 및 초기화.\n",
        "# 단순 선형 회귀이므로 input_dim = 1, output_dim - 1.\n",
        "# (1, 1) : 하나의 입력 x에 대해서 하나의 출력 y를 가진다.\n",
        "model = nn.Linear(1, 1)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghetYPuzoQyy",
        "outputId": "585cdc45-19f6-409b-ff20-bad7d96d46ee"
      },
      "source": [
        "# model에는 W, b가 저장되어있다. model.parameters()로 출력 가능\n",
        "print(list(model.parameters()))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.4414], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vw6lfxWohjZ"
      },
      "source": [
        "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) "
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQOu6-cRoqOG",
        "outputId": "e2450198-3ca5-416e-996f-55ad73c2bb8e"
      },
      "source": [
        "# 전체 훈련 데이터에 대해 경사 하강법을 2천회 반복\n",
        "nb_epochs = 2000\n",
        "\n",
        "for epoch in range(nb_epochs + 1) :\n",
        "\n",
        "  # H(x) 계산\n",
        "  prediction = model(x_train)\n",
        "\n",
        "  # cost 계산, 평균 제곱 오차 함수 사용\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "  # cost로 H(x) 개선\n",
        "  # gradient를 0으로 초기화\n",
        "  optimizer.zero_grad()\n",
        "  # 비용 함수를 미분하여 gradient 계산\n",
        "  cost.backward()\n",
        "  # W, b 업데이트\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch & 100 == 0 :\n",
        "    # 100번 마다 로그 출력\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 Cost: 13.103541\n",
            "Epoch    1/2000 Cost: 10.358571\n",
            "Epoch    2/2000 Cost: 8.188817\n",
            "Epoch    3/2000 Cost: 6.473738\n",
            "Epoch    8/2000 Cost: 2.000613\n",
            "Epoch    9/2000 Cost: 1.582267\n",
            "Epoch   10/2000 Cost: 1.251583\n",
            "Epoch   11/2000 Cost: 0.990191\n",
            "Epoch   16/2000 Cost: 0.308409\n",
            "Epoch   17/2000 Cost: 0.244637\n",
            "Epoch   18/2000 Cost: 0.194224\n",
            "Epoch   19/2000 Cost: 0.154372\n",
            "Epoch   24/2000 Cost: 0.050387\n",
            "Epoch   25/2000 Cost: 0.040652\n",
            "Epoch   26/2000 Cost: 0.032953\n",
            "Epoch   27/2000 Cost: 0.026864\n",
            "Epoch  128/2000 Cost: 0.002439\n",
            "Epoch  129/2000 Cost: 0.002427\n",
            "Epoch  130/2000 Cost: 0.002415\n",
            "Epoch  131/2000 Cost: 0.002404\n",
            "Epoch  136/2000 Cost: 0.002347\n",
            "Epoch  137/2000 Cost: 0.002335\n",
            "Epoch  138/2000 Cost: 0.002324\n",
            "Epoch  139/2000 Cost: 0.002313\n",
            "Epoch  144/2000 Cost: 0.002258\n",
            "Epoch  145/2000 Cost: 0.002247\n",
            "Epoch  146/2000 Cost: 0.002236\n",
            "Epoch  147/2000 Cost: 0.002226\n",
            "Epoch  152/2000 Cost: 0.002173\n",
            "Epoch  153/2000 Cost: 0.002162\n",
            "Epoch  154/2000 Cost: 0.002152\n",
            "Epoch  155/2000 Cost: 0.002141\n",
            "Epoch  256/2000 Cost: 0.001317\n",
            "Epoch  257/2000 Cost: 0.001311\n",
            "Epoch  258/2000 Cost: 0.001304\n",
            "Epoch  259/2000 Cost: 0.001298\n",
            "Epoch  264/2000 Cost: 0.001267\n",
            "Epoch  265/2000 Cost: 0.001261\n",
            "Epoch  266/2000 Cost: 0.001255\n",
            "Epoch  267/2000 Cost: 0.001249\n",
            "Epoch  272/2000 Cost: 0.001219\n",
            "Epoch  273/2000 Cost: 0.001213\n",
            "Epoch  274/2000 Cost: 0.001208\n",
            "Epoch  275/2000 Cost: 0.001202\n",
            "Epoch  280/2000 Cost: 0.001173\n",
            "Epoch  281/2000 Cost: 0.001168\n",
            "Epoch  282/2000 Cost: 0.001162\n",
            "Epoch  283/2000 Cost: 0.001156\n",
            "Epoch  384/2000 Cost: 0.000711\n",
            "Epoch  385/2000 Cost: 0.000708\n",
            "Epoch  386/2000 Cost: 0.000704\n",
            "Epoch  387/2000 Cost: 0.000701\n",
            "Epoch  392/2000 Cost: 0.000684\n",
            "Epoch  393/2000 Cost: 0.000681\n",
            "Epoch  394/2000 Cost: 0.000678\n",
            "Epoch  395/2000 Cost: 0.000675\n",
            "Epoch  400/2000 Cost: 0.000658\n",
            "Epoch  401/2000 Cost: 0.000655\n",
            "Epoch  402/2000 Cost: 0.000652\n",
            "Epoch  403/2000 Cost: 0.000649\n",
            "Epoch  408/2000 Cost: 0.000634\n",
            "Epoch  409/2000 Cost: 0.000631\n",
            "Epoch  410/2000 Cost: 0.000628\n",
            "Epoch  411/2000 Cost: 0.000625\n",
            "Epoch  512/2000 Cost: 0.000384\n",
            "Epoch  513/2000 Cost: 0.000382\n",
            "Epoch  514/2000 Cost: 0.000380\n",
            "Epoch  515/2000 Cost: 0.000379\n",
            "Epoch  520/2000 Cost: 0.000370\n",
            "Epoch  521/2000 Cost: 0.000368\n",
            "Epoch  522/2000 Cost: 0.000366\n",
            "Epoch  523/2000 Cost: 0.000364\n",
            "Epoch  528/2000 Cost: 0.000356\n",
            "Epoch  529/2000 Cost: 0.000354\n",
            "Epoch  530/2000 Cost: 0.000352\n",
            "Epoch  531/2000 Cost: 0.000350\n",
            "Epoch  536/2000 Cost: 0.000342\n",
            "Epoch  537/2000 Cost: 0.000341\n",
            "Epoch  538/2000 Cost: 0.000339\n",
            "Epoch  539/2000 Cost: 0.000337\n",
            "Epoch  640/2000 Cost: 0.000207\n",
            "Epoch  641/2000 Cost: 0.000206\n",
            "Epoch  642/2000 Cost: 0.000205\n",
            "Epoch  643/2000 Cost: 0.000204\n",
            "Epoch  648/2000 Cost: 0.000200\n",
            "Epoch  649/2000 Cost: 0.000199\n",
            "Epoch  650/2000 Cost: 0.000198\n",
            "Epoch  651/2000 Cost: 0.000197\n",
            "Epoch  656/2000 Cost: 0.000192\n",
            "Epoch  657/2000 Cost: 0.000191\n",
            "Epoch  658/2000 Cost: 0.000190\n",
            "Epoch  659/2000 Cost: 0.000189\n",
            "Epoch  664/2000 Cost: 0.000185\n",
            "Epoch  665/2000 Cost: 0.000184\n",
            "Epoch  666/2000 Cost: 0.000183\n",
            "Epoch  667/2000 Cost: 0.000182\n",
            "Epoch  768/2000 Cost: 0.000112\n",
            "Epoch  769/2000 Cost: 0.000111\n",
            "Epoch  770/2000 Cost: 0.000111\n",
            "Epoch  771/2000 Cost: 0.000110\n",
            "Epoch  776/2000 Cost: 0.000108\n",
            "Epoch  777/2000 Cost: 0.000107\n",
            "Epoch  778/2000 Cost: 0.000107\n",
            "Epoch  779/2000 Cost: 0.000106\n",
            "Epoch  784/2000 Cost: 0.000104\n",
            "Epoch  785/2000 Cost: 0.000103\n",
            "Epoch  786/2000 Cost: 0.000103\n",
            "Epoch  787/2000 Cost: 0.000102\n",
            "Epoch  792/2000 Cost: 0.000100\n",
            "Epoch  793/2000 Cost: 0.000099\n",
            "Epoch  794/2000 Cost: 0.000099\n",
            "Epoch  795/2000 Cost: 0.000098\n",
            "Epoch  896/2000 Cost: 0.000060\n",
            "Epoch  897/2000 Cost: 0.000060\n",
            "Epoch  898/2000 Cost: 0.000060\n",
            "Epoch  899/2000 Cost: 0.000060\n",
            "Epoch  904/2000 Cost: 0.000058\n",
            "Epoch  905/2000 Cost: 0.000058\n",
            "Epoch  906/2000 Cost: 0.000058\n",
            "Epoch  907/2000 Cost: 0.000057\n",
            "Epoch  912/2000 Cost: 0.000056\n",
            "Epoch  913/2000 Cost: 0.000056\n",
            "Epoch  914/2000 Cost: 0.000055\n",
            "Epoch  915/2000 Cost: 0.000055\n",
            "Epoch  920/2000 Cost: 0.000054\n",
            "Epoch  921/2000 Cost: 0.000054\n",
            "Epoch  922/2000 Cost: 0.000053\n",
            "Epoch  923/2000 Cost: 0.000053\n",
            "Epoch 1024/2000 Cost: 0.000033\n",
            "Epoch 1025/2000 Cost: 0.000033\n",
            "Epoch 1026/2000 Cost: 0.000032\n",
            "Epoch 1027/2000 Cost: 0.000032\n",
            "Epoch 1032/2000 Cost: 0.000031\n",
            "Epoch 1033/2000 Cost: 0.000031\n",
            "Epoch 1034/2000 Cost: 0.000031\n",
            "Epoch 1035/2000 Cost: 0.000031\n",
            "Epoch 1040/2000 Cost: 0.000030\n",
            "Epoch 1041/2000 Cost: 0.000030\n",
            "Epoch 1042/2000 Cost: 0.000030\n",
            "Epoch 1043/2000 Cost: 0.000030\n",
            "Epoch 1048/2000 Cost: 0.000029\n",
            "Epoch 1049/2000 Cost: 0.000029\n",
            "Epoch 1050/2000 Cost: 0.000029\n",
            "Epoch 1051/2000 Cost: 0.000029\n",
            "Epoch 1152/2000 Cost: 0.000018\n",
            "Epoch 1153/2000 Cost: 0.000018\n",
            "Epoch 1154/2000 Cost: 0.000017\n",
            "Epoch 1155/2000 Cost: 0.000017\n",
            "Epoch 1160/2000 Cost: 0.000017\n",
            "Epoch 1161/2000 Cost: 0.000017\n",
            "Epoch 1162/2000 Cost: 0.000017\n",
            "Epoch 1163/2000 Cost: 0.000017\n",
            "Epoch 1168/2000 Cost: 0.000016\n",
            "Epoch 1169/2000 Cost: 0.000016\n",
            "Epoch 1170/2000 Cost: 0.000016\n",
            "Epoch 1171/2000 Cost: 0.000016\n",
            "Epoch 1176/2000 Cost: 0.000016\n",
            "Epoch 1177/2000 Cost: 0.000016\n",
            "Epoch 1178/2000 Cost: 0.000016\n",
            "Epoch 1179/2000 Cost: 0.000015\n",
            "Epoch 1280/2000 Cost: 0.000010\n",
            "Epoch 1281/2000 Cost: 0.000009\n",
            "Epoch 1282/2000 Cost: 0.000009\n",
            "Epoch 1283/2000 Cost: 0.000009\n",
            "Epoch 1288/2000 Cost: 0.000009\n",
            "Epoch 1289/2000 Cost: 0.000009\n",
            "Epoch 1290/2000 Cost: 0.000009\n",
            "Epoch 1291/2000 Cost: 0.000009\n",
            "Epoch 1296/2000 Cost: 0.000009\n",
            "Epoch 1297/2000 Cost: 0.000009\n",
            "Epoch 1298/2000 Cost: 0.000009\n",
            "Epoch 1299/2000 Cost: 0.000009\n",
            "Epoch 1304/2000 Cost: 0.000008\n",
            "Epoch 1305/2000 Cost: 0.000008\n",
            "Epoch 1306/2000 Cost: 0.000008\n",
            "Epoch 1307/2000 Cost: 0.000008\n",
            "Epoch 1408/2000 Cost: 0.000005\n",
            "Epoch 1409/2000 Cost: 0.000005\n",
            "Epoch 1410/2000 Cost: 0.000005\n",
            "Epoch 1411/2000 Cost: 0.000005\n",
            "Epoch 1416/2000 Cost: 0.000005\n",
            "Epoch 1417/2000 Cost: 0.000005\n",
            "Epoch 1418/2000 Cost: 0.000005\n",
            "Epoch 1419/2000 Cost: 0.000005\n",
            "Epoch 1424/2000 Cost: 0.000005\n",
            "Epoch 1425/2000 Cost: 0.000005\n",
            "Epoch 1426/2000 Cost: 0.000005\n",
            "Epoch 1427/2000 Cost: 0.000005\n",
            "Epoch 1432/2000 Cost: 0.000005\n",
            "Epoch 1433/2000 Cost: 0.000005\n",
            "Epoch 1434/2000 Cost: 0.000005\n",
            "Epoch 1435/2000 Cost: 0.000005\n",
            "Epoch 1536/2000 Cost: 0.000003\n",
            "Epoch 1537/2000 Cost: 0.000003\n",
            "Epoch 1538/2000 Cost: 0.000003\n",
            "Epoch 1539/2000 Cost: 0.000003\n",
            "Epoch 1544/2000 Cost: 0.000003\n",
            "Epoch 1545/2000 Cost: 0.000003\n",
            "Epoch 1546/2000 Cost: 0.000003\n",
            "Epoch 1547/2000 Cost: 0.000003\n",
            "Epoch 1552/2000 Cost: 0.000003\n",
            "Epoch 1553/2000 Cost: 0.000003\n",
            "Epoch 1554/2000 Cost: 0.000003\n",
            "Epoch 1555/2000 Cost: 0.000003\n",
            "Epoch 1560/2000 Cost: 0.000002\n",
            "Epoch 1561/2000 Cost: 0.000002\n",
            "Epoch 1562/2000 Cost: 0.000002\n",
            "Epoch 1563/2000 Cost: 0.000002\n",
            "Epoch 1664/2000 Cost: 0.000002\n",
            "Epoch 1665/2000 Cost: 0.000001\n",
            "Epoch 1666/2000 Cost: 0.000001\n",
            "Epoch 1667/2000 Cost: 0.000001\n",
            "Epoch 1672/2000 Cost: 0.000001\n",
            "Epoch 1673/2000 Cost: 0.000001\n",
            "Epoch 1674/2000 Cost: 0.000001\n",
            "Epoch 1675/2000 Cost: 0.000001\n",
            "Epoch 1680/2000 Cost: 0.000001\n",
            "Epoch 1681/2000 Cost: 0.000001\n",
            "Epoch 1682/2000 Cost: 0.000001\n",
            "Epoch 1683/2000 Cost: 0.000001\n",
            "Epoch 1688/2000 Cost: 0.000001\n",
            "Epoch 1689/2000 Cost: 0.000001\n",
            "Epoch 1690/2000 Cost: 0.000001\n",
            "Epoch 1691/2000 Cost: 0.000001\n",
            "Epoch 1792/2000 Cost: 0.000001\n",
            "Epoch 1793/2000 Cost: 0.000001\n",
            "Epoch 1794/2000 Cost: 0.000001\n",
            "Epoch 1795/2000 Cost: 0.000001\n",
            "Epoch 1800/2000 Cost: 0.000001\n",
            "Epoch 1801/2000 Cost: 0.000001\n",
            "Epoch 1802/2000 Cost: 0.000001\n",
            "Epoch 1803/2000 Cost: 0.000001\n",
            "Epoch 1808/2000 Cost: 0.000001\n",
            "Epoch 1809/2000 Cost: 0.000001\n",
            "Epoch 1810/2000 Cost: 0.000001\n",
            "Epoch 1811/2000 Cost: 0.000001\n",
            "Epoch 1816/2000 Cost: 0.000001\n",
            "Epoch 1817/2000 Cost: 0.000001\n",
            "Epoch 1818/2000 Cost: 0.000001\n",
            "Epoch 1819/2000 Cost: 0.000001\n",
            "Epoch 1920/2000 Cost: 0.000000\n",
            "Epoch 1921/2000 Cost: 0.000000\n",
            "Epoch 1922/2000 Cost: 0.000000\n",
            "Epoch 1923/2000 Cost: 0.000000\n",
            "Epoch 1928/2000 Cost: 0.000000\n",
            "Epoch 1929/2000 Cost: 0.000000\n",
            "Epoch 1930/2000 Cost: 0.000000\n",
            "Epoch 1931/2000 Cost: 0.000000\n",
            "Epoch 1936/2000 Cost: 0.000000\n",
            "Epoch 1937/2000 Cost: 0.000000\n",
            "Epoch 1938/2000 Cost: 0.000000\n",
            "Epoch 1939/2000 Cost: 0.000000\n",
            "Epoch 1944/2000 Cost: 0.000000\n",
            "Epoch 1945/2000 Cost: 0.000000\n",
            "Epoch 1946/2000 Cost: 0.000000\n",
            "Epoch 1947/2000 Cost: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQKFU0xEp3v4"
      },
      "source": [
        "optimizer, hypothesis 과정을 생략하고 model()에 데이터를 직접 넣었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2Bqy-5Eoul4",
        "outputId": "9141a766-1c8f-43e6-d455-f0d9f3aa7c83"
      },
      "source": [
        "# 테스트해보기\n",
        "\n",
        "# 임의의 입력 4\n",
        "new_var = torch.FloatTensor([[4.0]])\n",
        "\n",
        "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
        "pred_y = model(new_var)\n",
        "\n",
        "# y = 2x 이므로 y = 8이면 제대로 학습된 것이다.\n",
        "print(pred_y)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[7.9989]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPUQyzg0qpGx",
        "outputId": "145c8e43-315e-421c-d603-6bab9bda12fb"
      },
      "source": [
        "# 학습 후 W와 b의 값을 출력해보자.\n",
        "print(list(model.parameters()))"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[1.9994]], requires_grad=True), Parameter containing:\n",
            "tensor([0.0014], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vhAiR5urA2X"
      },
      "source": [
        "다중 선형 회귀 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB7e0wqWqu76"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdWy_VGFrwU5",
        "outputId": "2064b06d-90a5-4147-a782-28469ec501d2"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4f868cb910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbk904vhrxMA"
      },
      "source": [
        "# 데이터\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HmSK0Jcrzaj"
      },
      "source": [
        "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
        "model = nn.Linear(3,1)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSJVTttHxQ0b"
      },
      "source": [
        "학습률은 1e-5(0.00001)로 한다. \n",
        "\n",
        "0.01로 하면 모델의 필요한 크기보다 높기 때문에 기울기가 발산한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFh9Q9Llr2ZY"
      },
      "source": [
        "# 학습률을 0.01로 하면 기울기는 발산\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) "
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29uqH1ycxKWl",
        "outputId": "b39b3f42-ba4d-4a8a-fee2-716c33714b5b"
      },
      "source": [
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
        "\n",
        "    # cost로 H(x) 개선하는 부분\n",
        "    # gradient를 0으로 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 비용 함수를 미분하여 gradient 계산\n",
        "    cost.backward()\n",
        "    # W와 b를 업데이트\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "    # 100번마다 로그 출력\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 Cost: 31667.597656\n",
            "Epoch  100/2000 Cost: nan\n",
            "Epoch  200/2000 Cost: nan\n",
            "Epoch  300/2000 Cost: nan\n",
            "Epoch  400/2000 Cost: nan\n",
            "Epoch  500/2000 Cost: nan\n",
            "Epoch  600/2000 Cost: nan\n",
            "Epoch  700/2000 Cost: nan\n",
            "Epoch  800/2000 Cost: nan\n",
            "Epoch  900/2000 Cost: nan\n",
            "Epoch 1000/2000 Cost: nan\n",
            "Epoch 1100/2000 Cost: nan\n",
            "Epoch 1200/2000 Cost: nan\n",
            "Epoch 1300/2000 Cost: nan\n",
            "Epoch 1400/2000 Cost: nan\n",
            "Epoch 1500/2000 Cost: nan\n",
            "Epoch 1600/2000 Cost: nan\n",
            "Epoch 1700/2000 Cost: nan\n",
            "Epoch 1800/2000 Cost: nan\n",
            "Epoch 1900/2000 Cost: nan\n",
            "Epoch 2000/2000 Cost: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcxlNGkTxMkU"
      },
      "source": [
        "# 기울기를 1e-5로 하면 ? 성공\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "model = nn.Linear(3,1)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9A9KhDGx-An",
        "outputId": "c096695c-a21d-427d-a4e7-3ca37c4bab7d"
      },
      "source": [
        "print(list(model.parameters()))"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[-0.5435,  0.3462, -0.1188]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2937], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjrQfhgxxghC",
        "outputId": "51685690-4b65-4534-80d5-0011209ab84f"
      },
      "source": [
        "nb_epochs = 20000\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
        "\n",
        "    # cost로 H(x) 개선하는 부분\n",
        "    # gradient를 0으로 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 비용 함수를 미분하여 gradient 계산\n",
        "    cost.backward()\n",
        "    # W와 b를 업데이트\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "    # 100번마다 로그 출력\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20000 Cost: 39633.414062\n",
            "Epoch  100/20000 Cost: 11.480746\n",
            "Epoch  200/20000 Cost: 10.894592\n",
            "Epoch  300/20000 Cost: 10.339335\n",
            "Epoch  400/20000 Cost: 9.813341\n",
            "Epoch  500/20000 Cost: 9.314991\n",
            "Epoch  600/20000 Cost: 8.842943\n",
            "Epoch  700/20000 Cost: 8.395709\n",
            "Epoch  800/20000 Cost: 7.972019\n",
            "Epoch  900/20000 Cost: 7.570613\n",
            "Epoch 1000/20000 Cost: 7.190366\n",
            "Epoch 1100/20000 Cost: 6.830102\n",
            "Epoch 1200/20000 Cost: 6.488800\n",
            "Epoch 1300/20000 Cost: 6.165435\n",
            "Epoch 1400/20000 Cost: 5.859081\n",
            "Epoch 1500/20000 Cost: 5.568858\n",
            "Epoch 1600/20000 Cost: 5.293914\n",
            "Epoch 1700/20000 Cost: 5.033381\n",
            "Epoch 1800/20000 Cost: 4.786536\n",
            "Epoch 1900/20000 Cost: 4.552677\n",
            "Epoch 2000/20000 Cost: 4.331120\n",
            "Epoch 2100/20000 Cost: 4.121175\n",
            "Epoch 2200/20000 Cost: 3.922304\n",
            "Epoch 2300/20000 Cost: 3.733839\n",
            "Epoch 2400/20000 Cost: 3.555275\n",
            "Epoch 2500/20000 Cost: 3.386094\n",
            "Epoch 2600/20000 Cost: 3.225775\n",
            "Epoch 2700/20000 Cost: 3.073856\n",
            "Epoch 2800/20000 Cost: 2.929915\n",
            "Epoch 2900/20000 Cost: 2.793523\n",
            "Epoch 3000/20000 Cost: 2.664274\n",
            "Epoch 3100/20000 Cost: 2.541785\n",
            "Epoch 3200/20000 Cost: 2.425741\n",
            "Epoch 3300/20000 Cost: 2.315737\n",
            "Epoch 3400/20000 Cost: 2.211524\n",
            "Epoch 3500/20000 Cost: 2.112742\n",
            "Epoch 3600/20000 Cost: 2.019123\n",
            "Epoch 3700/20000 Cost: 1.930390\n",
            "Epoch 3800/20000 Cost: 1.846296\n",
            "Epoch 3900/20000 Cost: 1.766624\n",
            "Epoch 4000/20000 Cost: 1.691073\n",
            "Epoch 4100/20000 Cost: 1.619471\n",
            "Epoch 4200/20000 Cost: 1.551607\n",
            "Epoch 4300/20000 Cost: 1.487279\n",
            "Epoch 4400/20000 Cost: 1.426291\n",
            "Epoch 4500/20000 Cost: 1.368487\n",
            "Epoch 4600/20000 Cost: 1.313681\n",
            "Epoch 4700/20000 Cost: 1.261732\n",
            "Epoch 4800/20000 Cost: 1.212463\n",
            "Epoch 4900/20000 Cost: 1.165762\n",
            "Epoch 5000/20000 Cost: 1.121496\n",
            "Epoch 5100/20000 Cost: 1.079494\n",
            "Epoch 5200/20000 Cost: 1.039670\n",
            "Epoch 5300/20000 Cost: 1.001922\n",
            "Epoch 5400/20000 Cost: 0.966094\n",
            "Epoch 5500/20000 Cost: 0.932144\n",
            "Epoch 5600/20000 Cost: 0.899911\n",
            "Epoch 5700/20000 Cost: 0.869357\n",
            "Epoch 5800/20000 Cost: 0.840379\n",
            "Epoch 5900/20000 Cost: 0.812889\n",
            "Epoch 6000/20000 Cost: 0.786795\n",
            "Epoch 6100/20000 Cost: 0.762042\n",
            "Epoch 6200/20000 Cost: 0.738554\n",
            "Epoch 6300/20000 Cost: 0.716265\n",
            "Epoch 6400/20000 Cost: 0.695110\n",
            "Epoch 6500/20000 Cost: 0.675017\n",
            "Epoch 6600/20000 Cost: 0.655970\n",
            "Epoch 6700/20000 Cost: 0.637870\n",
            "Epoch 6800/20000 Cost: 0.620698\n",
            "Epoch 6900/20000 Cost: 0.604379\n",
            "Epoch 7000/20000 Cost: 0.588885\n",
            "Epoch 7100/20000 Cost: 0.574179\n",
            "Epoch 7200/20000 Cost: 0.560203\n",
            "Epoch 7300/20000 Cost: 0.546922\n",
            "Epoch 7400/20000 Cost: 0.534310\n",
            "Epoch 7500/20000 Cost: 0.522320\n",
            "Epoch 7600/20000 Cost: 0.510927\n",
            "Epoch 7700/20000 Cost: 0.500084\n",
            "Epoch 7800/20000 Cost: 0.489794\n",
            "Epoch 7900/20000 Cost: 0.480001\n",
            "Epoch 8000/20000 Cost: 0.470683\n",
            "Epoch 8100/20000 Cost: 0.461828\n",
            "Epoch 8200/20000 Cost: 0.453399\n",
            "Epoch 8300/20000 Cost: 0.445377\n",
            "Epoch 8400/20000 Cost: 0.437735\n",
            "Epoch 8500/20000 Cost: 0.430473\n",
            "Epoch 8600/20000 Cost: 0.423548\n",
            "Epoch 8700/20000 Cost: 0.416961\n",
            "Epoch 8800/20000 Cost: 0.410672\n",
            "Epoch 8900/20000 Cost: 0.404696\n",
            "Epoch 9000/20000 Cost: 0.398982\n",
            "Epoch 9100/20000 Cost: 0.393544\n",
            "Epoch 9200/20000 Cost: 0.388356\n",
            "Epoch 9300/20000 Cost: 0.383397\n",
            "Epoch 9400/20000 Cost: 0.378679\n",
            "Epoch 9500/20000 Cost: 0.374173\n",
            "Epoch 9600/20000 Cost: 0.369875\n",
            "Epoch 9700/20000 Cost: 0.365752\n",
            "Epoch 9800/20000 Cost: 0.361828\n",
            "Epoch 9900/20000 Cost: 0.358057\n",
            "Epoch 10000/20000 Cost: 0.354469\n",
            "Epoch 10100/20000 Cost: 0.351031\n",
            "Epoch 10200/20000 Cost: 0.347735\n",
            "Epoch 10300/20000 Cost: 0.344588\n",
            "Epoch 10400/20000 Cost: 0.341571\n",
            "Epoch 10500/20000 Cost: 0.338683\n",
            "Epoch 10600/20000 Cost: 0.335904\n",
            "Epoch 10700/20000 Cost: 0.333248\n",
            "Epoch 10800/20000 Cost: 0.330699\n",
            "Epoch 10900/20000 Cost: 0.328246\n",
            "Epoch 11000/20000 Cost: 0.325896\n",
            "Epoch 11100/20000 Cost: 0.323629\n",
            "Epoch 11200/20000 Cost: 0.321461\n",
            "Epoch 11300/20000 Cost: 0.319370\n",
            "Epoch 11400/20000 Cost: 0.317356\n",
            "Epoch 11500/20000 Cost: 0.315424\n",
            "Epoch 11600/20000 Cost: 0.313553\n",
            "Epoch 11700/20000 Cost: 0.311751\n",
            "Epoch 11800/20000 Cost: 0.310017\n",
            "Epoch 11900/20000 Cost: 0.308342\n",
            "Epoch 12000/20000 Cost: 0.306722\n",
            "Epoch 12100/20000 Cost: 0.305162\n",
            "Epoch 12200/20000 Cost: 0.303646\n",
            "Epoch 12300/20000 Cost: 0.302189\n",
            "Epoch 12400/20000 Cost: 0.300775\n",
            "Epoch 12500/20000 Cost: 0.299397\n",
            "Epoch 12600/20000 Cost: 0.298074\n",
            "Epoch 12700/20000 Cost: 0.296783\n",
            "Epoch 12800/20000 Cost: 0.295534\n",
            "Epoch 12900/20000 Cost: 0.294318\n",
            "Epoch 13000/20000 Cost: 0.293145\n",
            "Epoch 13100/20000 Cost: 0.291997\n",
            "Epoch 13200/20000 Cost: 0.290878\n",
            "Epoch 13300/20000 Cost: 0.289796\n",
            "Epoch 13400/20000 Cost: 0.288738\n",
            "Epoch 13500/20000 Cost: 0.287710\n",
            "Epoch 13600/20000 Cost: 0.286706\n",
            "Epoch 13700/20000 Cost: 0.285724\n",
            "Epoch 13800/20000 Cost: 0.284764\n",
            "Epoch 13900/20000 Cost: 0.283832\n",
            "Epoch 14000/20000 Cost: 0.282922\n",
            "Epoch 14100/20000 Cost: 0.282029\n",
            "Epoch 14200/20000 Cost: 0.281157\n",
            "Epoch 14300/20000 Cost: 0.280300\n",
            "Epoch 14400/20000 Cost: 0.279463\n",
            "Epoch 14500/20000 Cost: 0.278642\n",
            "Epoch 14600/20000 Cost: 0.277842\n",
            "Epoch 14700/20000 Cost: 0.277050\n",
            "Epoch 14800/20000 Cost: 0.276275\n",
            "Epoch 14900/20000 Cost: 0.275509\n",
            "Epoch 15000/20000 Cost: 0.274769\n",
            "Epoch 15100/20000 Cost: 0.274034\n",
            "Epoch 15200/20000 Cost: 0.273312\n",
            "Epoch 15300/20000 Cost: 0.272607\n",
            "Epoch 15400/20000 Cost: 0.271902\n",
            "Epoch 15500/20000 Cost: 0.271209\n",
            "Epoch 15600/20000 Cost: 0.270537\n",
            "Epoch 15700/20000 Cost: 0.269866\n",
            "Epoch 15800/20000 Cost: 0.269207\n",
            "Epoch 15900/20000 Cost: 0.268559\n",
            "Epoch 16000/20000 Cost: 0.267917\n",
            "Epoch 16100/20000 Cost: 0.267287\n",
            "Epoch 16200/20000 Cost: 0.266664\n",
            "Epoch 16300/20000 Cost: 0.266049\n",
            "Epoch 16400/20000 Cost: 0.265435\n",
            "Epoch 16500/20000 Cost: 0.264836\n",
            "Epoch 16600/20000 Cost: 0.264239\n",
            "Epoch 16700/20000 Cost: 0.263654\n",
            "Epoch 16800/20000 Cost: 0.263074\n",
            "Epoch 16900/20000 Cost: 0.262497\n",
            "Epoch 17000/20000 Cost: 0.261929\n",
            "Epoch 17100/20000 Cost: 0.261365\n",
            "Epoch 17200/20000 Cost: 0.260814\n",
            "Epoch 17300/20000 Cost: 0.260257\n",
            "Epoch 17400/20000 Cost: 0.259709\n",
            "Epoch 17500/20000 Cost: 0.259170\n",
            "Epoch 17600/20000 Cost: 0.258635\n",
            "Epoch 17700/20000 Cost: 0.258103\n",
            "Epoch 17800/20000 Cost: 0.257578\n",
            "Epoch 17900/20000 Cost: 0.257054\n",
            "Epoch 18000/20000 Cost: 0.256536\n",
            "Epoch 18100/20000 Cost: 0.256023\n",
            "Epoch 18200/20000 Cost: 0.255515\n",
            "Epoch 18300/20000 Cost: 0.255012\n",
            "Epoch 18400/20000 Cost: 0.254506\n",
            "Epoch 18500/20000 Cost: 0.254015\n",
            "Epoch 18600/20000 Cost: 0.253515\n",
            "Epoch 18700/20000 Cost: 0.253033\n",
            "Epoch 18800/20000 Cost: 0.252547\n",
            "Epoch 18900/20000 Cost: 0.252064\n",
            "Epoch 19000/20000 Cost: 0.251586\n",
            "Epoch 19100/20000 Cost: 0.251104\n",
            "Epoch 19200/20000 Cost: 0.250639\n",
            "Epoch 19300/20000 Cost: 0.250165\n",
            "Epoch 19400/20000 Cost: 0.249701\n",
            "Epoch 19500/20000 Cost: 0.249234\n",
            "Epoch 19600/20000 Cost: 0.248770\n",
            "Epoch 19700/20000 Cost: 0.248321\n",
            "Epoch 19800/20000 Cost: 0.247870\n",
            "Epoch 19900/20000 Cost: 0.247414\n",
            "Epoch 20000/20000 Cost: 0.246958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pukI3RV8xj_-",
        "outputId": "185a83ac-fa8a-49d3-d03a-39ff9a3b4635"
      },
      "source": [
        "# 모델 테스트\n",
        "\n",
        "# 임의의 입력 [73, 80, 75]를 선언\n",
        "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
        "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
        "pred_y = model(new_var) \n",
        "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.3001]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ADM6nyUyau2",
        "outputId": "5111b04f-7d11-4bf9-889d-d9634b39b985"
      },
      "source": [
        "print(list(model.parameters()))"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.8882, 0.4473, 0.6713]], requires_grad=True), Parameter containing:\n",
            "tensor([0.3286], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}