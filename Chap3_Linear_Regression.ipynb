{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210417_선형회귀.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3/euVmyS1l6djI3v+T8ra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hodoosol/Start_DL_With_Pytorch/blob/main/Chap3_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CC9WulEaHWC"
      },
      "source": [
        "# 1. 선형 회귀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf9pCMdYTPM4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hURATlAYTBB4",
        "outputId": "106d4584-c813-4fdd-8a72-19377f33cce3"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4f868cb910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTx7xgzJS7QE"
      },
      "source": [
        "# 변수 선언\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMaBoXrkTTiw",
        "outputId": "1972645c-5189-4ddf-bfe0-a139f9118152"
      },
      "source": [
        "# 크기 출력\n",
        "print(x_train)\n",
        "print(x_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF3T7s14TeM_",
        "outputId": "23d74a4d-d8cf-48bb-ec0a-31574990f6d5"
      },
      "source": [
        "print(y_train)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.],\n",
            "        [4.],\n",
            "        [6.]])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JICe5cRbTgz2",
        "outputId": "2349457f-a00c-4684-c375-d3de6b5af611"
      },
      "source": [
        "# 가중치와 편향을 초기화해보자.\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "print(W)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtasfVMdTpg4",
        "outputId": "1dd2d4bc-faea-4b48-f650-1faf71d0d1fc"
      },
      "source": [
        "b = torch.zeros(1, requires_grad=True)\n",
        "print(b)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79p5A7zVTuj6"
      },
      "source": [
        "zeros를 사용해 가중치와 편향을 0으로 초기화 하고, \n",
        "\n",
        "requires_grad=True 를 통해 w와 b가 학습을 통해 값이 변경되는 변수임을 명시한다.\n",
        "\n",
        "현재 가중치 W와 b는 둘 다 0이므로 직선의 방정식은\n",
        "\n",
        "y = 0 * x + 0 이 된다. 적절하지 않다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI4xpldZTtkF",
        "outputId": "fdd80292-2c20-4c9f-c350-ac477201c3cd"
      },
      "source": [
        "# 가설 세우기\n",
        "hypothesis = x_train * W + b\n",
        "print(hypothesis)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3IoQxJUUcPM"
      },
      "source": [
        "수식적으로 단순히 '오차 = 실제값 - 예측값'으로 정의하면 오차값이 음수가 되는 경우가 생기므로 정확한 오차의 크기를 측정할 수 없다. \n",
        "\n",
        "따라서 오차를 그냥 전부 더하는 것이 아니라, \n",
        "\n",
        "각 오차들을 제곱해준 뒤에 전부 더하는 평균 제곱 오차(Mean Squared Error, MSE)로 비용함수를 만들어 보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1S4JUd4UxKz"
      },
      "source": [
        "기울기 w가 무한대로 커지면 커질 수록 cost의 값 또한 무한대로 커지고, 반대로 w가 무한대로 작아져도 cost의 값은 무한대로 커진다. \n",
        "\n",
        "따라서 경사하강법을 사용하여 비용 함수를 미분했을 때의 접선의 기울기가 0이 되는 지점을 찾아야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sJd1x94T7_W",
        "outputId": "42319411-bbc0-45d8-ffc4-da20f88d2af7"
      },
      "source": [
        "# 비용 함수 선언하기\n",
        "cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "print(cost)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8OCl91QVLXe"
      },
      "source": [
        "# 경사 하강법 구현하기\n",
        "## SGD는 경사 하강법의 일종이며 lr은 learning rate를 의미한다.\n",
        "## 학습 대상인 W와 b가 SGD의 입력이 된다.\n",
        "\n",
        "optimizer = optim.SGD([W, b], lr = 0.01)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvSK9hulVotX"
      },
      "source": [
        "# optimizer.zero_grad()를 실행함으로써 미분을 통해 얻은 기울기를 0응로 초기화 한다.\n",
        "# 기울기를 초기화해야지만 새로운 가중치 편향에 대한 새로운 기울기를 구할 수 있다.\n",
        "optimizer.zero_grad()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VyIyWSiVnsc"
      },
      "source": [
        "# 그 다음 cost.backward() 함수를 호출하면 W와 b에 대한 기울기가 계산된다.\n",
        "# 비용 함수를 미분하여 gradient 계산\n",
        "cost.backward()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUjlyti9V_q7"
      },
      "source": [
        "# optimizer.step()을 호출하여 인수로 들어갔던 W와 b에서\n",
        "# 리턴되는 변수들의 기울기에 학습률을 곱하여 빼줌으로써 업데이트한다.\n",
        "\n",
        "optimizer.step()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9Q-l3hWYes"
      },
      "source": [
        "전체 코드 정리\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wcM_i_VWR_3",
        "outputId": "30d39f33-348e-4a1a-fd40-5977b65ff760"
      },
      "source": [
        "# 데이터\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr = 0.01)\n",
        "\n",
        "# 원하는 만큼 경사 하강법 반복\n",
        "nb_epochs = 2000\n",
        "\n",
        "for epoch in range(nb_epochs + 1) :\n",
        "  # H(x) 계산\n",
        "  hypothesis = x_train * W + b\n",
        "\n",
        "  # cost 계산\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # 100번마다 로그 출력\n",
        "  if epoch % 100 == 0 :\n",
        "    print('Epoch {:4d} / {} W : {:.3f}, b{:.3f} Cost : {:.6f}'\n",
        "    .format(epoch, nb_epochs, W.item(), b.item(), cost.item()))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0 / 2000 W : 0.187, b0.080 Cost : 18.666666\n",
            "Epoch  100 / 2000 W : 1.746, b0.578 Cost : 0.048171\n",
            "Epoch  200 / 2000 W : 1.800, b0.454 Cost : 0.029767\n",
            "Epoch  300 / 2000 W : 1.843, b0.357 Cost : 0.018394\n",
            "Epoch  400 / 2000 W : 1.876, b0.281 Cost : 0.011366\n",
            "Epoch  500 / 2000 W : 1.903, b0.221 Cost : 0.007024\n",
            "Epoch  600 / 2000 W : 1.924, b0.174 Cost : 0.004340\n",
            "Epoch  700 / 2000 W : 1.940, b0.136 Cost : 0.002682\n",
            "Epoch  800 / 2000 W : 1.953, b0.107 Cost : 0.001657\n",
            "Epoch  900 / 2000 W : 1.963, b0.084 Cost : 0.001024\n",
            "Epoch 1000 / 2000 W : 1.971, b0.066 Cost : 0.000633\n",
            "Epoch 1100 / 2000 W : 1.977, b0.052 Cost : 0.000391\n",
            "Epoch 1200 / 2000 W : 1.982, b0.041 Cost : 0.000242\n",
            "Epoch 1300 / 2000 W : 1.986, b0.032 Cost : 0.000149\n",
            "Epoch 1400 / 2000 W : 1.989, b0.025 Cost : 0.000092\n",
            "Epoch 1500 / 2000 W : 1.991, b0.020 Cost : 0.000057\n",
            "Epoch 1600 / 2000 W : 1.993, b0.016 Cost : 0.000035\n",
            "Epoch 1700 / 2000 W : 1.995, b0.012 Cost : 0.000022\n",
            "Epoch 1800 / 2000 W : 1.996, b0.010 Cost : 0.000013\n",
            "Epoch 1900 / 2000 W : 1.997, b0.008 Cost : 0.000008\n",
            "Epoch 2000 / 2000 W : 1.997, b0.006 Cost : 0.000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W39f9EG5Y94h"
      },
      "source": [
        "optimizer.zero_grad() 가 필요한 이유 ?\n",
        "\n",
        "파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX1oEltjXu4o",
        "outputId": "862280c7-26c5-48dc-c7b2-612fa44e4235"
      },
      "source": [
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "nb_epochs = 20\n",
        "\n",
        "for epoch in range(nb_epochs + 1) :\n",
        "  z = 2 * w\n",
        "\n",
        "  z.backward()\n",
        "  print('수식을 w로 미분한 값 : {}'.format(w.grad))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 2.0\n",
            "수식을 w로 미분한 값 : 4.0\n",
            "수식을 w로 미분한 값 : 6.0\n",
            "수식을 w로 미분한 값 : 8.0\n",
            "수식을 w로 미분한 값 : 10.0\n",
            "수식을 w로 미분한 값 : 12.0\n",
            "수식을 w로 미분한 값 : 14.0\n",
            "수식을 w로 미분한 값 : 16.0\n",
            "수식을 w로 미분한 값 : 18.0\n",
            "수식을 w로 미분한 값 : 20.0\n",
            "수식을 w로 미분한 값 : 22.0\n",
            "수식을 w로 미분한 값 : 24.0\n",
            "수식을 w로 미분한 값 : 26.0\n",
            "수식을 w로 미분한 값 : 28.0\n",
            "수식을 w로 미분한 값 : 30.0\n",
            "수식을 w로 미분한 값 : 32.0\n",
            "수식을 w로 미분한 값 : 34.0\n",
            "수식을 w로 미분한 값 : 36.0\n",
            "수식을 w로 미분한 값 : 38.0\n",
            "수식을 w로 미분한 값 : 40.0\n",
            "수식을 w로 미분한 값 : 42.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_EQGPK-aLd2"
      },
      "source": [
        "# 2. 자동 미분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdQS0C-DaOSE"
      },
      "source": [
        "임의로 2w^2 + 5라는 식을 세워보고, w에 대해서 미분해보자.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LsJ970CZaGc"
      },
      "source": [
        "# 값이 2인 임의의 스칼라 텐서 w를 선언, 이 텐서에 대한 기울기를 저장\n",
        "w = torch.tensor(2.0, requires_grad=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3tmY-Djaf_Z"
      },
      "source": [
        "# 수식 정의하기\n",
        "y = w ** 2\n",
        "z = 2 * y + 5"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHcehovidVRl"
      },
      "source": [
        "# 수식 z를 w에 대해서 미분하기. backward()는 해당 수식의 w에 대한 기울기를 계산.\n",
        "z.backward()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXRDscl4dfb2",
        "outputId": "ad8f23f7-52f0-431e-8745-f3596ccec491"
      },
      "source": [
        "# w.grad를 출력하면 w가 속한 수식을 w로 미분한 값을 확인할 수 있다.\n",
        "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNr691mHdoXN",
        "outputId": "a134c976-e232-4722-c207-d07c07c77b1c"
      },
      "source": [
        "a = 2 * (w ** 2) + 5\n",
        "a.backward()\n",
        "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 16.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxzlxRFLeAE5"
      },
      "source": [
        "y = w ** 4\n",
        "d = 2 * y + 5\n",
        "d.backward()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXxa1ij1gdWZ",
        "outputId": "ea75bbde-1cb3-46ff-f9cd-2e2d60bc7d96"
      },
      "source": [
        "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 80.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8akx0Hi06WTq",
        "outputId": "1ac85439-fc0d-4e2e-f919-b52a00918a64"
      },
      "source": [
        "w79 = torch.tensor(2.0, requires_grad=True)\n",
        "y79 = w79 ** 2\n",
        "z79 = 2 * y79 + 5\n",
        "\n",
        "z79.backward()\n",
        "print('수식을 w로 미분한 값 : {}'.format(w79.grad))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw3kAgfb6kZD",
        "outputId": "1284c31e-79c5-479d-c83c-34a072cbef04"
      },
      "source": [
        "w80 = torch.tensor(2.0, requires_grad=True)\n",
        "y80 = w80 ** 2\n",
        "z80 = 2 * (w80 ** 2) + 5\n",
        "\n",
        "z80.backward()\n",
        "print('수식을 w로 미분한 값 : {}'.format(w80.grad))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값 : 8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJYkk-9hhhfv"
      },
      "source": [
        "# 3. 다중 선형 회귀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPsGU7dTh8iQ"
      },
      "source": [
        "다수의 x로부터 y를 예측하는 다중 선형 회귀를 공부해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abKLopOXiSC5"
      },
      "source": [
        "3개의 퀴즈 점수로부터 최종 점수를 예측하는 모델 만들기.\n",
        "\n",
        "독립 변수 x의 개수가 3개 이므로\n",
        "\n",
        "H(x) = w1x1 + w2x2 + w3x3 + b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edQBRffDgzBq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmgWZPFzibKK",
        "outputId": "3c9f1af0-e64e-4b98-b1ed-16f9395ba69b"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4f868cb910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR0dkGrKid0X"
      },
      "source": [
        "# 훈련 데이터\n",
        "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
        "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
        "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ_66dX9ijdk"
      },
      "source": [
        "# 가중치 w와 b를 선언. w도 3개 선언해야 한다.\n",
        "w1 = torch.zeros(1, requires_grad=True)\n",
        "w2 = torch.zeros(1, requires_grad=True)\n",
        "w3 = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZECEiBzKiwfo",
        "outputId": "a56caab1-29a9-4acb-a2a7-44cdbe06475a"
      },
      "source": [
        "# 가설, 비용 함수, 옵티마이저를 선언한 후에 경사 하강법을 1천회 반복한다.\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
        "        ))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
            "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n",
            "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n",
            "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n",
            "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n",
            "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n",
            "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
            "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n",
            "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n",
            "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
            "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpEhgfnSlH-y"
      },
      "source": [
        "그러나 x의 개수가 3개가 아닌 1000개라면 ??\n",
        "\n",
        "x_train1 ... x_train1000 을 전부 선언하고\n",
        "\n",
        "w1 ... w1000 을 전부 선언해야 한다.\n",
        "\n",
        "이를 해결하기 위해 행렬 곱셈 연산(벡터의 내적)을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipWBx-g5jp04"
      },
      "source": [
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  80], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6s8ZOFhmBCi",
        "outputId": "8699e2bb-9ce5-4bad-c9e4-c864c428bf6b"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3])\n",
            "torch.Size([5, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdM-VRT_mDsv"
      },
      "source": [
        "# 가중치와 편향 선언\n",
        "W = torch.zeros((3, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXco4AiRmOcA"
      },
      "source": [
        "행렬의 곱셈이 성립되려면\n",
        "\n",
        "좌측에 있는 행렬의 열 크기 == 우측에 있는 행렬의 행 크기\n",
        "\n",
        "x_train은 (5, 3)dlrh w는 (3, 1)이므로 가능해진다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL7L5ZZpmKNt"
      },
      "source": [
        "# 가설 선언하기 _ 행렬곱 matmul 사용\n",
        "hypothesis = x_train.matmul(W) + b"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpewyHFkm28U"
      },
      "source": [
        "전체 코드 정리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxIN2KMimqPn",
        "outputId": "1c6451b3-0315-402f-d995-7711d5039573"
      },
      "source": [
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  80], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros((3, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.SGD([W, b], lr = 1e-5)\n",
        "\n",
        "nb_epochs = 20\n",
        "\n",
        "for epoch in range(nb_epochs + 1) :\n",
        "  # H(x) 계산\n",
        "  # 편향 b는 브로드 캐스팅되어 각 샘플에 더해진다.\n",
        "  hypothesis = x_train.matmul(W) + b\n",
        "  \n",
        "  # cost 계산\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
        "    ))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
            "Epoch    1/20 hypothesis: tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]) Cost: 9537.694336\n",
            "Epoch    2/20 hypothesis: tensor([104.5421, 125.6208, 119.2478, 134.7861,  95.8280]) Cost: 3069.590820\n",
            "Epoch    3/20 hypothesis: tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]) Cost: 990.670288\n",
            "Epoch    4/20 hypothesis: tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]) Cost: 322.481873\n",
            "Epoch    5/20 hypothesis: tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]) Cost: 107.717064\n",
            "Epoch    6/20 hypothesis: tensor([148.9423, 178.9730, 169.8976, 192.0301, 136.5279]) Cost: 38.687496\n",
            "Epoch    7/20 hypothesis: tensor([151.1574, 181.6346, 172.4254, 194.8856, 138.5585]) Cost: 16.499043\n",
            "Epoch    8/20 hypothesis: tensor([152.4131, 183.1435, 173.8590, 196.5043, 139.7097]) Cost: 9.365656\n",
            "Epoch    9/20 hypothesis: tensor([153.1250, 183.9988, 174.6723, 197.4217, 140.3625]) Cost: 7.071114\n",
            "Epoch   10/20 hypothesis: tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]) Cost: 6.331847\n",
            "Epoch   11/20 hypothesis: tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]) Cost: 6.092532\n",
            "Epoch   12/20 hypothesis: tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0613]) Cost: 6.013817\n",
            "Epoch   13/20 hypothesis: tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]) Cost: 5.986785\n",
            "Epoch   14/20 hypothesis: tensor([154.0017, 185.0517, 175.6785, 198.5500, 141.1671]) Cost: 5.976325\n",
            "Epoch   15/20 hypothesis: tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]) Cost: 5.971208\n",
            "Epoch   16/20 hypothesis: tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]) Cost: 5.967835\n",
            "Epoch   17/20 hypothesis: tensor([154.0459, 185.1045, 175.7326, 198.6059, 141.2082]) Cost: 5.964969\n",
            "Epoch   18/20 hypothesis: tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]) Cost: 5.962291\n",
            "Epoch   19/20 hypothesis: tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]) Cost: 5.959664\n",
            "Epoch   20/20 hypothesis: tensor([154.0536, 185.1134, 175.7451, 198.6145, 141.2158]) Cost: 5.957089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us6aHUW9nke-"
      },
      "source": [
        "# 4. nn.Module로 구현하는 선형 회귀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3xLob2CncyM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puu2Gn_qnxeW",
        "outputId": "a8607bae-f87f-4a55-a9cf-e94161d34fa3"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4f868cb910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhUySIvdn2lH"
      },
      "source": [
        "# y = 2x를 가정한 상태에서 만들어진 데이터 (정답 W = 2, b = 0)\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RUnHAIsn-eV"
      },
      "source": [
        "# 선형 회귀 모델 구현 _ 모델 선언 및 초기화.\n",
        "# 단순 선형 회귀이므로 input_dim = 1, output_dim - 1.\n",
        "# (1, 1) : 하나의 입력 x에 대해서 하나의 출력 y를 가진다.\n",
        "model = nn.Linear(1, 1)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghetYPuzoQyy",
        "outputId": "217b3b20-7346-4fa9-f683-88fea291c1de"
      },
      "source": [
        "# model에는 W, b가 저장되어있다. model.parameters()로 출력 가능\n",
        "print(list(model.parameters()))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[-0.1939]], requires_grad=True), Parameter containing:\n",
            "tensor([0.4694], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vw6lfxWohjZ"
      },
      "source": [
        "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) "
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQOu6-cRoqOG",
        "outputId": "34090d55-b485-41cd-d643-af156b33f5c1"
      },
      "source": [
        "# 전체 훈련 데이터에 대해 경사 하강법을 2천회 반복\n",
        "nb_epochs = 2000\n",
        "\n",
        "for epoch in range(nb_epochs + 1) :\n",
        "\n",
        "  # H(x) 계산\n",
        "  prediction = model(x_train)\n",
        "\n",
        "  # cost 계산, 평균 제곱 오차 함수 사용\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "  # cost로 H(x) 개선\n",
        "  # gradient를 0으로 초기화\n",
        "  optimizer.zero_grad()\n",
        "  # 비용 함수를 미분하여 gradient 계산\n",
        "  cost.backward()\n",
        "  # W, b 업데이트\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch & 100 == 0 :\n",
        "    # 100번 마다 로그 출력\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 Cost: 18.562185\n",
            "Epoch    1/2000 Cost: 14.714844\n",
            "Epoch    2/2000 Cost: 11.673522\n",
            "Epoch    3/2000 Cost: 9.269318\n",
            "Epoch    8/2000 Cost: 2.996658\n",
            "Epoch    9/2000 Cost: 2.409524\n",
            "Epoch   10/2000 Cost: 1.945230\n",
            "Epoch   11/2000 Cost: 1.578036\n",
            "Epoch   16/2000 Cost: 0.618160\n",
            "Epoch   17/2000 Cost: 0.527906\n",
            "Epoch   18/2000 Cost: 0.456377\n",
            "Epoch   19/2000 Cost: 0.399650\n",
            "Epoch   24/2000 Cost: 0.249581\n",
            "Epoch   25/2000 Cost: 0.235080\n",
            "Epoch   26/2000 Cost: 0.223437\n",
            "Epoch   27/2000 Cost: 0.214053\n",
            "Epoch  128/2000 Cost: 0.111904\n",
            "Epoch  129/2000 Cost: 0.111367\n",
            "Epoch  130/2000 Cost: 0.110832\n",
            "Epoch  131/2000 Cost: 0.110300\n",
            "Epoch  136/2000 Cost: 0.107677\n",
            "Epoch  137/2000 Cost: 0.107160\n",
            "Epoch  138/2000 Cost: 0.106645\n",
            "Epoch  139/2000 Cost: 0.106133\n",
            "Epoch  144/2000 Cost: 0.103609\n",
            "Epoch  145/2000 Cost: 0.103112\n",
            "Epoch  146/2000 Cost: 0.102617\n",
            "Epoch  147/2000 Cost: 0.102124\n",
            "Epoch  152/2000 Cost: 0.099695\n",
            "Epoch  153/2000 Cost: 0.099217\n",
            "Epoch  154/2000 Cost: 0.098740\n",
            "Epoch  155/2000 Cost: 0.098266\n",
            "Epoch  256/2000 Cost: 0.060431\n",
            "Epoch  257/2000 Cost: 0.060141\n",
            "Epoch  258/2000 Cost: 0.059852\n",
            "Epoch  259/2000 Cost: 0.059564\n",
            "Epoch  264/2000 Cost: 0.058148\n",
            "Epoch  265/2000 Cost: 0.057869\n",
            "Epoch  266/2000 Cost: 0.057591\n",
            "Epoch  267/2000 Cost: 0.057314\n",
            "Epoch  272/2000 Cost: 0.055951\n",
            "Epoch  273/2000 Cost: 0.055683\n",
            "Epoch  274/2000 Cost: 0.055415\n",
            "Epoch  275/2000 Cost: 0.055149\n",
            "Epoch  280/2000 Cost: 0.053838\n",
            "Epoch  281/2000 Cost: 0.053579\n",
            "Epoch  282/2000 Cost: 0.053322\n",
            "Epoch  283/2000 Cost: 0.053066\n",
            "Epoch  384/2000 Cost: 0.032634\n",
            "Epoch  385/2000 Cost: 0.032477\n",
            "Epoch  386/2000 Cost: 0.032321\n",
            "Epoch  387/2000 Cost: 0.032166\n",
            "Epoch  392/2000 Cost: 0.031401\n",
            "Epoch  393/2000 Cost: 0.031250\n",
            "Epoch  394/2000 Cost: 0.031100\n",
            "Epoch  395/2000 Cost: 0.030951\n",
            "Epoch  400/2000 Cost: 0.030215\n",
            "Epoch  401/2000 Cost: 0.030070\n",
            "Epoch  402/2000 Cost: 0.029925\n",
            "Epoch  403/2000 Cost: 0.029782\n",
            "Epoch  408/2000 Cost: 0.029073\n",
            "Epoch  409/2000 Cost: 0.028934\n",
            "Epoch  410/2000 Cost: 0.028795\n",
            "Epoch  411/2000 Cost: 0.028657\n",
            "Epoch  512/2000 Cost: 0.017623\n",
            "Epoch  513/2000 Cost: 0.017538\n",
            "Epoch  514/2000 Cost: 0.017454\n",
            "Epoch  515/2000 Cost: 0.017370\n",
            "Epoch  520/2000 Cost: 0.016957\n",
            "Epoch  521/2000 Cost: 0.016876\n",
            "Epoch  522/2000 Cost: 0.016795\n",
            "Epoch  523/2000 Cost: 0.016714\n",
            "Epoch  528/2000 Cost: 0.016317\n",
            "Epoch  529/2000 Cost: 0.016238\n",
            "Epoch  530/2000 Cost: 0.016160\n",
            "Epoch  531/2000 Cost: 0.016083\n",
            "Epoch  536/2000 Cost: 0.015700\n",
            "Epoch  537/2000 Cost: 0.015625\n",
            "Epoch  538/2000 Cost: 0.015550\n",
            "Epoch  539/2000 Cost: 0.015475\n",
            "Epoch  640/2000 Cost: 0.009517\n",
            "Epoch  641/2000 Cost: 0.009471\n",
            "Epoch  642/2000 Cost: 0.009426\n",
            "Epoch  643/2000 Cost: 0.009380\n",
            "Epoch  648/2000 Cost: 0.009157\n",
            "Epoch  649/2000 Cost: 0.009113\n",
            "Epoch  650/2000 Cost: 0.009070\n",
            "Epoch  651/2000 Cost: 0.009026\n",
            "Epoch  656/2000 Cost: 0.008811\n",
            "Epoch  657/2000 Cost: 0.008769\n",
            "Epoch  658/2000 Cost: 0.008727\n",
            "Epoch  659/2000 Cost: 0.008685\n",
            "Epoch  664/2000 Cost: 0.008478\n",
            "Epoch  665/2000 Cost: 0.008438\n",
            "Epoch  666/2000 Cost: 0.008397\n",
            "Epoch  667/2000 Cost: 0.008357\n",
            "Epoch  768/2000 Cost: 0.005139\n",
            "Epoch  769/2000 Cost: 0.005115\n",
            "Epoch  770/2000 Cost: 0.005090\n",
            "Epoch  771/2000 Cost: 0.005066\n",
            "Epoch  776/2000 Cost: 0.004945\n",
            "Epoch  777/2000 Cost: 0.004921\n",
            "Epoch  778/2000 Cost: 0.004898\n",
            "Epoch  779/2000 Cost: 0.004874\n",
            "Epoch  784/2000 Cost: 0.004758\n",
            "Epoch  785/2000 Cost: 0.004735\n",
            "Epoch  786/2000 Cost: 0.004713\n",
            "Epoch  787/2000 Cost: 0.004690\n",
            "Epoch  792/2000 Cost: 0.004579\n",
            "Epoch  793/2000 Cost: 0.004557\n",
            "Epoch  794/2000 Cost: 0.004535\n",
            "Epoch  795/2000 Cost: 0.004513\n",
            "Epoch  896/2000 Cost: 0.002775\n",
            "Epoch  897/2000 Cost: 0.002762\n",
            "Epoch  898/2000 Cost: 0.002749\n",
            "Epoch  899/2000 Cost: 0.002736\n",
            "Epoch  904/2000 Cost: 0.002670\n",
            "Epoch  905/2000 Cost: 0.002658\n",
            "Epoch  906/2000 Cost: 0.002645\n",
            "Epoch  907/2000 Cost: 0.002632\n",
            "Epoch  912/2000 Cost: 0.002570\n",
            "Epoch  913/2000 Cost: 0.002557\n",
            "Epoch  914/2000 Cost: 0.002545\n",
            "Epoch  915/2000 Cost: 0.002533\n",
            "Epoch  920/2000 Cost: 0.002472\n",
            "Epoch  921/2000 Cost: 0.002461\n",
            "Epoch  922/2000 Cost: 0.002449\n",
            "Epoch  923/2000 Cost: 0.002437\n",
            "Epoch 1024/2000 Cost: 0.001499\n",
            "Epoch 1025/2000 Cost: 0.001492\n",
            "Epoch 1026/2000 Cost: 0.001484\n",
            "Epoch 1027/2000 Cost: 0.001477\n",
            "Epoch 1032/2000 Cost: 0.001442\n",
            "Epoch 1033/2000 Cost: 0.001435\n",
            "Epoch 1034/2000 Cost: 0.001428\n",
            "Epoch 1035/2000 Cost: 0.001421\n",
            "Epoch 1040/2000 Cost: 0.001388\n",
            "Epoch 1041/2000 Cost: 0.001381\n",
            "Epoch 1042/2000 Cost: 0.001374\n",
            "Epoch 1043/2000 Cost: 0.001368\n",
            "Epoch 1048/2000 Cost: 0.001335\n",
            "Epoch 1049/2000 Cost: 0.001329\n",
            "Epoch 1050/2000 Cost: 0.001322\n",
            "Epoch 1051/2000 Cost: 0.001316\n",
            "Epoch 1152/2000 Cost: 0.000809\n",
            "Epoch 1153/2000 Cost: 0.000805\n",
            "Epoch 1154/2000 Cost: 0.000802\n",
            "Epoch 1155/2000 Cost: 0.000798\n",
            "Epoch 1160/2000 Cost: 0.000779\n",
            "Epoch 1161/2000 Cost: 0.000775\n",
            "Epoch 1162/2000 Cost: 0.000771\n",
            "Epoch 1163/2000 Cost: 0.000768\n",
            "Epoch 1168/2000 Cost: 0.000749\n",
            "Epoch 1169/2000 Cost: 0.000746\n",
            "Epoch 1170/2000 Cost: 0.000742\n",
            "Epoch 1171/2000 Cost: 0.000739\n",
            "Epoch 1176/2000 Cost: 0.000721\n",
            "Epoch 1177/2000 Cost: 0.000718\n",
            "Epoch 1178/2000 Cost: 0.000714\n",
            "Epoch 1179/2000 Cost: 0.000711\n",
            "Epoch 1280/2000 Cost: 0.000437\n",
            "Epoch 1281/2000 Cost: 0.000435\n",
            "Epoch 1282/2000 Cost: 0.000433\n",
            "Epoch 1283/2000 Cost: 0.000431\n",
            "Epoch 1288/2000 Cost: 0.000421\n",
            "Epoch 1289/2000 Cost: 0.000419\n",
            "Epoch 1290/2000 Cost: 0.000417\n",
            "Epoch 1291/2000 Cost: 0.000415\n",
            "Epoch 1296/2000 Cost: 0.000405\n",
            "Epoch 1297/2000 Cost: 0.000403\n",
            "Epoch 1298/2000 Cost: 0.000401\n",
            "Epoch 1299/2000 Cost: 0.000399\n",
            "Epoch 1304/2000 Cost: 0.000389\n",
            "Epoch 1305/2000 Cost: 0.000388\n",
            "Epoch 1306/2000 Cost: 0.000386\n",
            "Epoch 1307/2000 Cost: 0.000384\n",
            "Epoch 1408/2000 Cost: 0.000236\n",
            "Epoch 1409/2000 Cost: 0.000235\n",
            "Epoch 1410/2000 Cost: 0.000234\n",
            "Epoch 1411/2000 Cost: 0.000233\n",
            "Epoch 1416/2000 Cost: 0.000227\n",
            "Epoch 1417/2000 Cost: 0.000226\n",
            "Epoch 1418/2000 Cost: 0.000225\n",
            "Epoch 1419/2000 Cost: 0.000224\n",
            "Epoch 1424/2000 Cost: 0.000219\n",
            "Epoch 1425/2000 Cost: 0.000217\n",
            "Epoch 1426/2000 Cost: 0.000216\n",
            "Epoch 1427/2000 Cost: 0.000215\n",
            "Epoch 1432/2000 Cost: 0.000210\n",
            "Epoch 1433/2000 Cost: 0.000209\n",
            "Epoch 1434/2000 Cost: 0.000208\n",
            "Epoch 1435/2000 Cost: 0.000207\n",
            "Epoch 1536/2000 Cost: 0.000127\n",
            "Epoch 1537/2000 Cost: 0.000127\n",
            "Epoch 1538/2000 Cost: 0.000126\n",
            "Epoch 1539/2000 Cost: 0.000126\n",
            "Epoch 1544/2000 Cost: 0.000123\n",
            "Epoch 1545/2000 Cost: 0.000122\n",
            "Epoch 1546/2000 Cost: 0.000121\n",
            "Epoch 1547/2000 Cost: 0.000121\n",
            "Epoch 1552/2000 Cost: 0.000118\n",
            "Epoch 1553/2000 Cost: 0.000117\n",
            "Epoch 1554/2000 Cost: 0.000117\n",
            "Epoch 1555/2000 Cost: 0.000116\n",
            "Epoch 1560/2000 Cost: 0.000114\n",
            "Epoch 1561/2000 Cost: 0.000113\n",
            "Epoch 1562/2000 Cost: 0.000112\n",
            "Epoch 1563/2000 Cost: 0.000112\n",
            "Epoch 1664/2000 Cost: 0.000069\n",
            "Epoch 1665/2000 Cost: 0.000068\n",
            "Epoch 1666/2000 Cost: 0.000068\n",
            "Epoch 1667/2000 Cost: 0.000068\n",
            "Epoch 1672/2000 Cost: 0.000066\n",
            "Epoch 1673/2000 Cost: 0.000066\n",
            "Epoch 1674/2000 Cost: 0.000066\n",
            "Epoch 1675/2000 Cost: 0.000065\n",
            "Epoch 1680/2000 Cost: 0.000064\n",
            "Epoch 1681/2000 Cost: 0.000063\n",
            "Epoch 1682/2000 Cost: 0.000063\n",
            "Epoch 1683/2000 Cost: 0.000063\n",
            "Epoch 1688/2000 Cost: 0.000061\n",
            "Epoch 1689/2000 Cost: 0.000061\n",
            "Epoch 1690/2000 Cost: 0.000061\n",
            "Epoch 1691/2000 Cost: 0.000060\n",
            "Epoch 1792/2000 Cost: 0.000037\n",
            "Epoch 1793/2000 Cost: 0.000037\n",
            "Epoch 1794/2000 Cost: 0.000037\n",
            "Epoch 1795/2000 Cost: 0.000037\n",
            "Epoch 1800/2000 Cost: 0.000036\n",
            "Epoch 1801/2000 Cost: 0.000036\n",
            "Epoch 1802/2000 Cost: 0.000035\n",
            "Epoch 1803/2000 Cost: 0.000035\n",
            "Epoch 1808/2000 Cost: 0.000034\n",
            "Epoch 1809/2000 Cost: 0.000034\n",
            "Epoch 1810/2000 Cost: 0.000034\n",
            "Epoch 1811/2000 Cost: 0.000034\n",
            "Epoch 1816/2000 Cost: 0.000033\n",
            "Epoch 1817/2000 Cost: 0.000033\n",
            "Epoch 1818/2000 Cost: 0.000033\n",
            "Epoch 1819/2000 Cost: 0.000033\n",
            "Epoch 1920/2000 Cost: 0.000020\n",
            "Epoch 1921/2000 Cost: 0.000020\n",
            "Epoch 1922/2000 Cost: 0.000020\n",
            "Epoch 1923/2000 Cost: 0.000020\n",
            "Epoch 1928/2000 Cost: 0.000019\n",
            "Epoch 1929/2000 Cost: 0.000019\n",
            "Epoch 1930/2000 Cost: 0.000019\n",
            "Epoch 1931/2000 Cost: 0.000019\n",
            "Epoch 1936/2000 Cost: 0.000019\n",
            "Epoch 1937/2000 Cost: 0.000018\n",
            "Epoch 1938/2000 Cost: 0.000018\n",
            "Epoch 1939/2000 Cost: 0.000018\n",
            "Epoch 1944/2000 Cost: 0.000018\n",
            "Epoch 1945/2000 Cost: 0.000018\n",
            "Epoch 1946/2000 Cost: 0.000018\n",
            "Epoch 1947/2000 Cost: 0.000018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQKFU0xEp3v4"
      },
      "source": [
        "optimizer, hypothesis 과정을 생략하고 model()에 데이터를 직접 넣었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2Bqy-5Eoul4",
        "outputId": "5a4e9abc-8349-468c-d118-91731fd39c2d"
      },
      "source": [
        "# 테스트해보기\n",
        "\n",
        "# 임의의 입력 4\n",
        "new_var = torch.FloatTensor([[4.0]])\n",
        "\n",
        "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
        "pred_y = model(new_var)\n",
        "\n",
        "# y = 2x 이므로 y = 8이면 제대로 학습된 것이다.\n",
        "print(pred_y)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[7.9926]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPUQyzg0qpGx",
        "outputId": "206eca5c-dd20-4cdb-e74d-508976c09c81"
      },
      "source": [
        "# 학습 후 W와 b의 값을 출력해보자.\n",
        "print(list(model.parameters()))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[1.9957]], requires_grad=True), Parameter containing:\n",
            "tensor([0.0097], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vhAiR5urA2X"
      },
      "source": [
        "다중 선형 회귀 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB7e0wqWqu76"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdWy_VGFrwU5",
        "outputId": "2620d74d-af36-451a-9f6a-232ada2f9138"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4f868cb910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbk904vhrxMA"
      },
      "source": [
        "# 데이터\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HmSK0Jcrzaj"
      },
      "source": [
        "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
        "model = nn.Linear(3,1)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSJVTttHxQ0b"
      },
      "source": [
        "학습률은 1e-5(0.00001)로 한다. \n",
        "\n",
        "0.01로 하면 모델의 필요한 크기보다 높기 때문에 기울기가 발산한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFh9Q9Llr2ZY"
      },
      "source": [
        "# 학습률을 0.01로 하면 기울기는 발산\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) "
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29uqH1ycxKWl",
        "outputId": "9f860166-e36f-4ac3-815e-7f9da3cdd825"
      },
      "source": [
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
        "\n",
        "    # cost로 H(x) 개선하는 부분\n",
        "    # gradient를 0으로 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 비용 함수를 미분하여 gradient 계산\n",
        "    cost.backward()\n",
        "    # W와 b를 업데이트\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "    # 100번마다 로그 출력\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 Cost: 31667.597656\n",
            "Epoch  100/2000 Cost: nan\n",
            "Epoch  200/2000 Cost: nan\n",
            "Epoch  300/2000 Cost: nan\n",
            "Epoch  400/2000 Cost: nan\n",
            "Epoch  500/2000 Cost: nan\n",
            "Epoch  600/2000 Cost: nan\n",
            "Epoch  700/2000 Cost: nan\n",
            "Epoch  800/2000 Cost: nan\n",
            "Epoch  900/2000 Cost: nan\n",
            "Epoch 1000/2000 Cost: nan\n",
            "Epoch 1100/2000 Cost: nan\n",
            "Epoch 1200/2000 Cost: nan\n",
            "Epoch 1300/2000 Cost: nan\n",
            "Epoch 1400/2000 Cost: nan\n",
            "Epoch 1500/2000 Cost: nan\n",
            "Epoch 1600/2000 Cost: nan\n",
            "Epoch 1700/2000 Cost: nan\n",
            "Epoch 1800/2000 Cost: nan\n",
            "Epoch 1900/2000 Cost: nan\n",
            "Epoch 2000/2000 Cost: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcxlNGkTxMkU"
      },
      "source": [
        "# 기울기를 1e-5로 하면 ? 성공\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "model = nn.Linear(3,1)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9A9KhDGx-An",
        "outputId": "60804dad-f44f-4c9d-d5a9-f65a5f17d414"
      },
      "source": [
        "print(list(model.parameters()))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.4789, 0.8222, 0.7059]], requires_grad=True), Parameter containing:\n",
            "tensor([0.3070], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjrQfhgxxghC",
        "outputId": "2974d74e-d4d8-444a-925b-1c3f93056b80"
      },
      "source": [
        "nb_epochs = 20000\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
        "\n",
        "    # cost로 H(x) 개선하는 부분\n",
        "    # gradient를 0으로 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 비용 함수를 미분하여 gradient 계산\n",
        "    cost.backward()\n",
        "    # W와 b를 업데이트\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "    # 100번마다 로그 출력\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20000 Cost: 0.202571\n",
            "Epoch  100/20000 Cost: 0.202349\n",
            "Epoch  200/20000 Cost: 0.202126\n",
            "Epoch  300/20000 Cost: 0.201910\n",
            "Epoch  400/20000 Cost: 0.201690\n",
            "Epoch  500/20000 Cost: 0.201473\n",
            "Epoch  600/20000 Cost: 0.201258\n",
            "Epoch  700/20000 Cost: 0.201052\n",
            "Epoch  800/20000 Cost: 0.200841\n",
            "Epoch  900/20000 Cost: 0.200626\n",
            "Epoch 1000/20000 Cost: 0.200415\n",
            "Epoch 1100/20000 Cost: 0.200199\n",
            "Epoch 1200/20000 Cost: 0.200002\n",
            "Epoch 1300/20000 Cost: 0.199787\n",
            "Epoch 1400/20000 Cost: 0.199582\n",
            "Epoch 1500/20000 Cost: 0.199377\n",
            "Epoch 1600/20000 Cost: 0.199172\n",
            "Epoch 1700/20000 Cost: 0.198972\n",
            "Epoch 1800/20000 Cost: 0.198766\n",
            "Epoch 1900/20000 Cost: 0.198563\n",
            "Epoch 2000/20000 Cost: 0.198364\n",
            "Epoch 2100/20000 Cost: 0.198168\n",
            "Epoch 2200/20000 Cost: 0.197961\n",
            "Epoch 2300/20000 Cost: 0.197767\n",
            "Epoch 2400/20000 Cost: 0.197571\n",
            "Epoch 2500/20000 Cost: 0.197377\n",
            "Epoch 2600/20000 Cost: 0.197182\n",
            "Epoch 2700/20000 Cost: 0.196993\n",
            "Epoch 2800/20000 Cost: 0.196793\n",
            "Epoch 2900/20000 Cost: 0.196603\n",
            "Epoch 3000/20000 Cost: 0.196412\n",
            "Epoch 3100/20000 Cost: 0.196224\n",
            "Epoch 3200/20000 Cost: 0.196031\n",
            "Epoch 3300/20000 Cost: 0.195840\n",
            "Epoch 3400/20000 Cost: 0.195651\n",
            "Epoch 3500/20000 Cost: 0.195469\n",
            "Epoch 3600/20000 Cost: 0.195281\n",
            "Epoch 3700/20000 Cost: 0.195098\n",
            "Epoch 3800/20000 Cost: 0.194908\n",
            "Epoch 3900/20000 Cost: 0.194732\n",
            "Epoch 4000/20000 Cost: 0.194549\n",
            "Epoch 4100/20000 Cost: 0.194367\n",
            "Epoch 4200/20000 Cost: 0.194189\n",
            "Epoch 4300/20000 Cost: 0.194012\n",
            "Epoch 4400/20000 Cost: 0.193830\n",
            "Epoch 4500/20000 Cost: 0.193649\n",
            "Epoch 4600/20000 Cost: 0.193474\n",
            "Epoch 4700/20000 Cost: 0.193300\n",
            "Epoch 4800/20000 Cost: 0.193122\n",
            "Epoch 4900/20000 Cost: 0.192954\n",
            "Epoch 5000/20000 Cost: 0.192778\n",
            "Epoch 5100/20000 Cost: 0.192605\n",
            "Epoch 5200/20000 Cost: 0.192436\n",
            "Epoch 5300/20000 Cost: 0.192263\n",
            "Epoch 5400/20000 Cost: 0.192093\n",
            "Epoch 5500/20000 Cost: 0.191920\n",
            "Epoch 5600/20000 Cost: 0.191755\n",
            "Epoch 5700/20000 Cost: 0.191585\n",
            "Epoch 5800/20000 Cost: 0.191416\n",
            "Epoch 5900/20000 Cost: 0.191257\n",
            "Epoch 6000/20000 Cost: 0.191087\n",
            "Epoch 6100/20000 Cost: 0.190923\n",
            "Epoch 6200/20000 Cost: 0.190759\n",
            "Epoch 6300/20000 Cost: 0.190595\n",
            "Epoch 6400/20000 Cost: 0.190436\n",
            "Epoch 6500/20000 Cost: 0.190270\n",
            "Epoch 6600/20000 Cost: 0.190113\n",
            "Epoch 6700/20000 Cost: 0.189948\n",
            "Epoch 6800/20000 Cost: 0.189793\n",
            "Epoch 6900/20000 Cost: 0.189628\n",
            "Epoch 7000/20000 Cost: 0.189478\n",
            "Epoch 7100/20000 Cost: 0.189322\n",
            "Epoch 7200/20000 Cost: 0.189161\n",
            "Epoch 7300/20000 Cost: 0.189008\n",
            "Epoch 7400/20000 Cost: 0.188857\n",
            "Epoch 7500/20000 Cost: 0.188701\n",
            "Epoch 7600/20000 Cost: 0.188546\n",
            "Epoch 7700/20000 Cost: 0.188395\n",
            "Epoch 7800/20000 Cost: 0.188241\n",
            "Epoch 7900/20000 Cost: 0.188092\n",
            "Epoch 8000/20000 Cost: 0.187946\n",
            "Epoch 8100/20000 Cost: 0.187793\n",
            "Epoch 8200/20000 Cost: 0.187641\n",
            "Epoch 8300/20000 Cost: 0.187500\n",
            "Epoch 8400/20000 Cost: 0.187349\n",
            "Epoch 8500/20000 Cost: 0.187203\n",
            "Epoch 8600/20000 Cost: 0.187058\n",
            "Epoch 8700/20000 Cost: 0.186912\n",
            "Epoch 8800/20000 Cost: 0.186768\n",
            "Epoch 8900/20000 Cost: 0.186626\n",
            "Epoch 9000/20000 Cost: 0.186480\n",
            "Epoch 9100/20000 Cost: 0.186337\n",
            "Epoch 9200/20000 Cost: 0.186197\n",
            "Epoch 9300/20000 Cost: 0.186054\n",
            "Epoch 9400/20000 Cost: 0.185917\n",
            "Epoch 9500/20000 Cost: 0.185778\n",
            "Epoch 9600/20000 Cost: 0.185638\n",
            "Epoch 9700/20000 Cost: 0.185500\n",
            "Epoch 9800/20000 Cost: 0.185361\n",
            "Epoch 9900/20000 Cost: 0.185226\n",
            "Epoch 10000/20000 Cost: 0.185092\n",
            "Epoch 10100/20000 Cost: 0.184953\n",
            "Epoch 10200/20000 Cost: 0.184820\n",
            "Epoch 10300/20000 Cost: 0.184685\n",
            "Epoch 10400/20000 Cost: 0.184548\n",
            "Epoch 10500/20000 Cost: 0.184419\n",
            "Epoch 10600/20000 Cost: 0.184283\n",
            "Epoch 10700/20000 Cost: 0.184154\n",
            "Epoch 10800/20000 Cost: 0.184017\n",
            "Epoch 10900/20000 Cost: 0.183893\n",
            "Epoch 11000/20000 Cost: 0.183756\n",
            "Epoch 11100/20000 Cost: 0.183629\n",
            "Epoch 11200/20000 Cost: 0.183505\n",
            "Epoch 11300/20000 Cost: 0.183378\n",
            "Epoch 11400/20000 Cost: 0.183245\n",
            "Epoch 11500/20000 Cost: 0.183122\n",
            "Epoch 11600/20000 Cost: 0.182991\n",
            "Epoch 11700/20000 Cost: 0.182867\n",
            "Epoch 11800/20000 Cost: 0.182741\n",
            "Epoch 11900/20000 Cost: 0.182622\n",
            "Epoch 12000/20000 Cost: 0.182501\n",
            "Epoch 12100/20000 Cost: 0.182373\n",
            "Epoch 12200/20000 Cost: 0.182252\n",
            "Epoch 12300/20000 Cost: 0.182129\n",
            "Epoch 12400/20000 Cost: 0.182009\n",
            "Epoch 12500/20000 Cost: 0.181886\n",
            "Epoch 12600/20000 Cost: 0.181769\n",
            "Epoch 12700/20000 Cost: 0.181650\n",
            "Epoch 12800/20000 Cost: 0.181525\n",
            "Epoch 12900/20000 Cost: 0.181409\n",
            "Epoch 13000/20000 Cost: 0.181290\n",
            "Epoch 13100/20000 Cost: 0.181178\n",
            "Epoch 13200/20000 Cost: 0.181058\n",
            "Epoch 13300/20000 Cost: 0.180944\n",
            "Epoch 13400/20000 Cost: 0.180831\n",
            "Epoch 13500/20000 Cost: 0.180708\n",
            "Epoch 13600/20000 Cost: 0.180592\n",
            "Epoch 13700/20000 Cost: 0.180485\n",
            "Epoch 13800/20000 Cost: 0.180366\n",
            "Epoch 13900/20000 Cost: 0.180257\n",
            "Epoch 14000/20000 Cost: 0.180139\n",
            "Epoch 14100/20000 Cost: 0.180030\n",
            "Epoch 14200/20000 Cost: 0.179920\n",
            "Epoch 14300/20000 Cost: 0.179812\n",
            "Epoch 14400/20000 Cost: 0.179701\n",
            "Epoch 14500/20000 Cost: 0.179589\n",
            "Epoch 14600/20000 Cost: 0.179481\n",
            "Epoch 14700/20000 Cost: 0.179368\n",
            "Epoch 14800/20000 Cost: 0.179261\n",
            "Epoch 14900/20000 Cost: 0.179156\n",
            "Epoch 15000/20000 Cost: 0.179050\n",
            "Epoch 15100/20000 Cost: 0.178941\n",
            "Epoch 15200/20000 Cost: 0.178835\n",
            "Epoch 15300/20000 Cost: 0.178727\n",
            "Epoch 15400/20000 Cost: 0.178626\n",
            "Epoch 15500/20000 Cost: 0.178521\n",
            "Epoch 15600/20000 Cost: 0.178419\n",
            "Epoch 15700/20000 Cost: 0.178309\n",
            "Epoch 15800/20000 Cost: 0.178208\n",
            "Epoch 15900/20000 Cost: 0.178105\n",
            "Epoch 16000/20000 Cost: 0.178007\n",
            "Epoch 16100/20000 Cost: 0.177902\n",
            "Epoch 16200/20000 Cost: 0.177802\n",
            "Epoch 16300/20000 Cost: 0.177704\n",
            "Epoch 16400/20000 Cost: 0.177602\n",
            "Epoch 16500/20000 Cost: 0.177503\n",
            "Epoch 16600/20000 Cost: 0.177402\n",
            "Epoch 16700/20000 Cost: 0.177306\n",
            "Epoch 16800/20000 Cost: 0.177213\n",
            "Epoch 16900/20000 Cost: 0.177107\n",
            "Epoch 17000/20000 Cost: 0.177014\n",
            "Epoch 17100/20000 Cost: 0.176914\n",
            "Epoch 17200/20000 Cost: 0.176818\n",
            "Epoch 17300/20000 Cost: 0.176721\n",
            "Epoch 17400/20000 Cost: 0.176627\n",
            "Epoch 17500/20000 Cost: 0.176532\n",
            "Epoch 17600/20000 Cost: 0.176442\n",
            "Epoch 17700/20000 Cost: 0.176345\n",
            "Epoch 17800/20000 Cost: 0.176251\n",
            "Epoch 17900/20000 Cost: 0.176159\n",
            "Epoch 18000/20000 Cost: 0.176065\n",
            "Epoch 18100/20000 Cost: 0.175976\n",
            "Epoch 18200/20000 Cost: 0.175882\n",
            "Epoch 18300/20000 Cost: 0.175792\n",
            "Epoch 18400/20000 Cost: 0.175698\n",
            "Epoch 18500/20000 Cost: 0.175603\n",
            "Epoch 18600/20000 Cost: 0.175518\n",
            "Epoch 18700/20000 Cost: 0.175427\n",
            "Epoch 18800/20000 Cost: 0.175338\n",
            "Epoch 18900/20000 Cost: 0.175249\n",
            "Epoch 19000/20000 Cost: 0.175164\n",
            "Epoch 19100/20000 Cost: 0.175071\n",
            "Epoch 19200/20000 Cost: 0.174988\n",
            "Epoch 19300/20000 Cost: 0.174902\n",
            "Epoch 19400/20000 Cost: 0.174814\n",
            "Epoch 19500/20000 Cost: 0.174732\n",
            "Epoch 19600/20000 Cost: 0.174644\n",
            "Epoch 19700/20000 Cost: 0.174556\n",
            "Epoch 19800/20000 Cost: 0.174477\n",
            "Epoch 19900/20000 Cost: 0.174383\n",
            "Epoch 20000/20000 Cost: 0.174303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pukI3RV8xj_-",
        "outputId": "fb6c6d38-5917-4194-afea-b05f3c1f94bf"
      },
      "source": [
        "# 모델 테스트\n",
        "\n",
        "# 임의의 입력 [73, 80, 75]를 선언\n",
        "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
        "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
        "pred_y = model(new_var) \n",
        "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.4299]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ADM6nyUyau2",
        "outputId": "2bbfdd84-848f-4006-d395-a8887ffc9a37"
      },
      "source": [
        "print(list(model.parameters()))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.9719, 0.4846, 0.5514]], requires_grad=True), Parameter containing:\n",
            "tensor([0.3523], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95OnlNHuyljC"
      },
      "source": [
        "# 5. 클래스로 파이토치 모델 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5CTjkLiyg28"
      },
      "source": [
        "# 단순 선형 회귀 모델은 model = nn.Linear(1, 1)로 구현했었다.\n",
        "# 동일 모델을 클래스로 구현해보자.\n",
        "\n",
        "# torch.nn.Module을 상속받는 파이썬 클래스\n",
        "class LinearRegressionModel(nn.Module) :\n",
        "  def __init__(self) :\n",
        "    super().__init__()\n",
        "    # 단순 선형 회귀이므로 input_dim = 1, output_dim = 1\n",
        "    self.linear = nn.Linear(1, 1)\n",
        "\n",
        "  def forward(self, x) :\n",
        "    return self.linear(x)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQe9dkeAzEac"
      },
      "source": [
        "model = LinearRegressionModel()"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sZH1ad2zKZO"
      },
      "source": [
        "# 다중 선형 회귀 모델은 model == nn.Linear(3, 1)로 구현했었다.\n",
        "# 동일 모델을 클래스로 구현해보자.\n",
        "\n",
        "class MultivariateLinearRegressionModel(nn.Module) :\n",
        "  def __init__(self) :\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3, 1)\n",
        "\n",
        "  def forward(self, x) :\n",
        "    return self.linear(x)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08ZxD1Ia0JoP"
      },
      "source": [
        "model = MultivariateLinearRegressionModel()"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YRosJzt0UrY"
      },
      "source": [
        "# 6. 미니 배치와 데이터 로드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NLKYrab9WH6"
      },
      "source": [
        "데이터가 굉장히 많을 때, 예를 들어 수십만개 이상이라면 전체 데이터에 대해서\n",
        "\n",
        "경사 하강법을 수행하는 것은 매우 느릴 뿐만 아니라 많은 계산량이 필요하다. \n",
        "\n",
        "어쩌면 메모리의 한계로 불가능할 수도.\n",
        "\n",
        "이 문제를 해결하기 위해 전체 데이터를 더 작은 단위로 나누어서 학습하는 미니 배치를 사용할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov0R-O9h0LUT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pl923tQ-Rjf"
      },
      "source": [
        "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
        "from torch.utils.data import DataLoader # 데이터로더"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHTQSJOt-Tvt"
      },
      "source": [
        "# 데이터 입력\n",
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  90], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhsG4n9k-ZKr"
      },
      "source": [
        "# 위의 데이터를 TensorDataset의 입력으로 사용하기 위해 dataset에 저장하자.\n",
        "dataset = TensorDataset(x_train, y_train)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91RMjzy3-tcL"
      },
      "source": [
        "이제 파이토치의 데이터 세트를 만들었기 때문에 데이터 로더를 사용할 수 있다.\n",
        "\n",
        "데이터 로더는 기본적으로 데이터셋, 미니 배치의 크기 - > 2개의 인자를 입력받는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQAFk_iM-jUN"
      },
      "source": [
        "dataloader = DataLoader(dataset, batch_size = 2, shuffle=True)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kseCmTUe-_UZ"
      },
      "source": [
        "# 모델과 옵티마이저를 설계한다.\n",
        "model = nn.Linear(3, 1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSS_X4sp_H3_",
        "outputId": "ec27b0e1-2056-4526-d101-70f6155754f3"
      },
      "source": [
        "# 훈련하기 !\n",
        "nb_epochs = 20\n",
        "\n",
        "for epoch in range(nb_epochs + 1) :\n",
        "  for batch_idx, samples in enumerate(dataloader) :\n",
        "    # print(batch_idx)\n",
        "    # print(samples)\n",
        "    x_train, y_train = samples\n",
        "    \n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    # cost로 H(x) 계산\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
        "        cost.item()\n",
        "        ))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Batch 1/3 Cost: 0.480877\n",
            "Epoch    0/20 Batch 2/3 Cost: 2.749199\n",
            "Epoch    0/20 Batch 3/3 Cost: 1.903234\n",
            "Epoch    1/20 Batch 1/3 Cost: 1.941651\n",
            "Epoch    1/20 Batch 2/3 Cost: 1.142585\n",
            "Epoch    1/20 Batch 3/3 Cost: 3.566102\n",
            "Epoch    2/20 Batch 1/3 Cost: 3.169156\n",
            "Epoch    2/20 Batch 2/3 Cost: 2.139808\n",
            "Epoch    2/20 Batch 3/3 Cost: 1.125601\n",
            "Epoch    3/20 Batch 1/3 Cost: 0.712265\n",
            "Epoch    3/20 Batch 2/3 Cost: 2.042973\n",
            "Epoch    3/20 Batch 3/3 Cost: 2.996371\n",
            "Epoch    4/20 Batch 1/3 Cost: 1.390489\n",
            "Epoch    4/20 Batch 2/3 Cost: 2.063843\n",
            "Epoch    4/20 Batch 3/3 Cost: 1.879563\n",
            "Epoch    5/20 Batch 1/3 Cost: 1.542500\n",
            "Epoch    5/20 Batch 2/3 Cost: 2.538434\n",
            "Epoch    5/20 Batch 3/3 Cost: 0.639810\n",
            "Epoch    6/20 Batch 1/3 Cost: 2.550020\n",
            "Epoch    6/20 Batch 2/3 Cost: 0.667031\n",
            "Epoch    6/20 Batch 3/3 Cost: 2.796340\n",
            "Epoch    7/20 Batch 1/3 Cost: 2.543345\n",
            "Epoch    7/20 Batch 2/3 Cost: 1.021681\n",
            "Epoch    7/20 Batch 3/3 Cost: 0.479373\n",
            "Epoch    8/20 Batch 1/3 Cost: 1.997885\n",
            "Epoch    8/20 Batch 2/3 Cost: 1.522404\n",
            "Epoch    8/20 Batch 3/3 Cost: 2.609574\n",
            "Epoch    9/20 Batch 1/3 Cost: 1.614267\n",
            "Epoch    9/20 Batch 2/3 Cost: 3.106426\n",
            "Epoch    9/20 Batch 3/3 Cost: 1.819716\n",
            "Epoch   10/20 Batch 1/3 Cost: 0.317661\n",
            "Epoch   10/20 Batch 2/3 Cost: 1.538739\n",
            "Epoch   10/20 Batch 3/3 Cost: 4.175280\n",
            "Epoch   11/20 Batch 1/3 Cost: 2.607874\n",
            "Epoch   11/20 Batch 2/3 Cost: 3.196692\n",
            "Epoch   11/20 Batch 3/3 Cost: 0.859559\n",
            "Epoch   12/20 Batch 1/3 Cost: 2.180875\n",
            "Epoch   12/20 Batch 2/3 Cost: 2.549932\n",
            "Epoch   12/20 Batch 3/3 Cost: 0.638687\n",
            "Epoch   13/20 Batch 1/3 Cost: 2.452944\n",
            "Epoch   13/20 Batch 2/3 Cost: 0.973048\n",
            "Epoch   13/20 Batch 3/3 Cost: 2.966178\n",
            "Epoch   14/20 Batch 1/3 Cost: 1.028274\n",
            "Epoch   14/20 Batch 2/3 Cost: 1.682973\n",
            "Epoch   14/20 Batch 3/3 Cost: 2.700469\n",
            "Epoch   15/20 Batch 1/3 Cost: 3.823435\n",
            "Epoch   15/20 Batch 2/3 Cost: 1.060774\n",
            "Epoch   15/20 Batch 3/3 Cost: 1.188391\n",
            "Epoch   16/20 Batch 1/3 Cost: 2.973063\n",
            "Epoch   16/20 Batch 2/3 Cost: 1.178881\n",
            "Epoch   16/20 Batch 3/3 Cost: 0.679437\n",
            "Epoch   17/20 Batch 1/3 Cost: 2.682002\n",
            "Epoch   17/20 Batch 2/3 Cost: 1.144828\n",
            "Epoch   17/20 Batch 3/3 Cost: 0.244960\n",
            "Epoch   18/20 Batch 1/3 Cost: 0.243956\n",
            "Epoch   18/20 Batch 2/3 Cost: 2.697992\n",
            "Epoch   18/20 Batch 3/3 Cost: 1.911031\n",
            "Epoch   19/20 Batch 1/3 Cost: 1.865361\n",
            "Epoch   19/20 Batch 2/3 Cost: 1.118827\n",
            "Epoch   19/20 Batch 3/3 Cost: 3.495113\n",
            "Epoch   20/20 Batch 1/3 Cost: 0.979282\n",
            "Epoch   20/20 Batch 2/3 Cost: 2.794004\n",
            "Epoch   20/20 Batch 3/3 Cost: 2.355861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq7cqY2OAJMR",
        "outputId": "6a7849f6-7b23-45f1-f696-1daf1ad0c4b7"
      },
      "source": [
        "# 모델 테스트\n",
        "\n",
        "# 임의의 입력 [73, 80, 75]를 선언\n",
        "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
        "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
        "pred_y = model(new_var) \n",
        "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[152.0837]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krx4etabA3l0"
      },
      "source": [
        "# 7. 커스텀 데이터셋"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4llRK58Bj44"
      },
      "source": [
        "커스텀 데이터셋을 만들 때, 가장 기본적인 뼈대는"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz_Fz85-A1WT"
      },
      "source": [
        "# class CustomDataset(torch.utils.data.Dataset) :\n",
        "#   def __init__(self) :\n",
        "#     데이터셋의 전처리를 해준다.\n",
        "  \n",
        "#   def __len__(self) :\n",
        "#     데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분.\n",
        "\n",
        "#   def __getitem__(self, idx) :\n",
        "#     데이터셋에서 특정 1개의 샘플을 가져오는 함수이다."
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAEWXQKGCNco"
      },
      "source": [
        "커스텀 데이터셋으로 선형 회귀 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CugFmJzvB_vn"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cBD72WoCSCY"
      },
      "source": [
        "# 데이터셋 상속\n",
        "class CustomDataset(Dataset) :\n",
        "  def __init__(self) :\n",
        "    self.x_data = [[73, 80, 75],\n",
        "                   [93, 88, 93],\n",
        "                   [89, 91, 90],\n",
        "                   [96, 98, 100],\n",
        "                   [73, 66, 70]]\n",
        "    self.y_data = [[152], [185], [180], [196], [142]]\n",
        "\n",
        "  # 총 데이터의 개수를 리턴\n",
        "  def __len__(self) :\n",
        "    return len(self.x_data)\n",
        "\n",
        "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
        "  def __getitem__(self, idx) :\n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "    return x, y\n",
        "  "
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXKSnm6FDCTu"
      },
      "source": [
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx4xtuK9DIpD"
      },
      "source": [
        "model = torch.nn.Linear(3, 1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S6Poj00DO1Q",
        "outputId": "d85f19cf-85fa-4313-b99b-7f463b1a5119"
      },
      "source": [
        "# 학습 시작\n",
        "\n",
        "nb_epochs = 20\n",
        "\n",
        "for epoch in range(nb_epochs + 1) :\n",
        "  x_train, y_train = samples\n",
        "\n",
        "  prediction = model(x_train)\n",
        "\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
        "        cost.item()\n",
        "        ))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Batch 3/3 Cost: 4097.970703\n",
            "Epoch    1/20 Batch 3/3 Cost: 735.258301\n",
            "Epoch    2/20 Batch 3/3 Cost: 131.919968\n",
            "Epoch    3/20 Batch 3/3 Cost: 23.669168\n",
            "Epoch    4/20 Batch 3/3 Cost: 4.246734\n",
            "Epoch    5/20 Batch 3/3 Cost: 0.761918\n",
            "Epoch    6/20 Batch 3/3 Cost: 0.136716\n",
            "Epoch    7/20 Batch 3/3 Cost: 0.024524\n",
            "Epoch    8/20 Batch 3/3 Cost: 0.004400\n",
            "Epoch    9/20 Batch 3/3 Cost: 0.000789\n",
            "Epoch   10/20 Batch 3/3 Cost: 0.000141\n",
            "Epoch   11/20 Batch 3/3 Cost: 0.000026\n",
            "Epoch   12/20 Batch 3/3 Cost: 0.000004\n",
            "Epoch   13/20 Batch 3/3 Cost: 0.000001\n",
            "Epoch   14/20 Batch 3/3 Cost: 0.000000\n",
            "Epoch   15/20 Batch 3/3 Cost: 0.000000\n",
            "Epoch   16/20 Batch 3/3 Cost: 0.000000\n",
            "Epoch   17/20 Batch 3/3 Cost: 0.000000\n",
            "Epoch   18/20 Batch 3/3 Cost: 0.000000\n",
            "Epoch   19/20 Batch 3/3 Cost: 0.000000\n",
            "Epoch   20/20 Batch 3/3 Cost: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBCK4M8FDt5L",
        "outputId": "64c73a69-55bf-4eae-d11e-3e0be96b9886"
      },
      "source": [
        "# 모델 테스트\n",
        "\n",
        "# 임의의 입력 [73, 80, 75]를 선언\n",
        "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
        "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
        "pred_y = model(new_var) \n",
        "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.7448]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}